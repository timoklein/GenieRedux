diff --git a/deep_rl_zoo/agent57/eval_agent.py b/deep_rl_zoo/agent57/eval_agent.py
index e49a337..0bca2b2 100644
--- a/deep_rl_zoo/agent57/eval_agent.py
+++ b/deep_rl_zoo/agent57/eval_agent.py
@@ -22,9 +22,9 @@ import torch
 # pylint: disable=import-error
 from deep_rl_zoo.networks.value import Agent57Conv2dNet
 from deep_rl_zoo.networks.curiosity import RndConvNet, NguEmbeddingConvNet
-from deep_rl_zoo import main_loop
+from deep_rl_zoo import main_loop_gymnasium as main_loop
 from deep_rl_zoo.checkpoint import PyTorchCheckpoint
-from deep_rl_zoo import gym_env
+from deep_rl_zoo import gym_env_gymnasium as gym_env
 from deep_rl_zoo import greedy_actors
 
 
@@ -52,6 +52,8 @@ flags.DEFINE_integer('max_episode_steps', 58000, 'Maximum steps (before frame sk
 flags.DEFINE_integer('seed', 1, 'Runtime seed.')
 flags.DEFINE_bool('use_tensorboard', True, 'Use Tensorboard to monitor statistics, default on.')
 flags.DEFINE_string('load_checkpoint_file', '', 'Load a specific checkpoint file.')
+flags.DEFINE_string('root_dir', '/scratch/nedko_savov/projects/ivg/external/open-genie/agent57/runs_eval', 'Root directory for storing results.')
+flags.DEFINE_string('reward', 'combined', 'Reward type.')
 flags.DEFINE_string(
     'recording_video_dir',
     'recordings',
@@ -61,10 +63,27 @@ flags.DEFINE_string(
 flags.register_validator('environment_frame_stack', lambda x: x == 1)
 
 
+def environment_builder(environment_name, environment_height, environment_width, environment_frame_skip, environment_frame_stack, max_episode_steps, seed, reward_type):
+    return gym_env.create_retro_environment(
+        env_name=environment_name,
+        frame_height=environment_height,
+        frame_width=environment_width,
+        frame_skip=environment_frame_skip,
+        frame_stack=environment_frame_stack,
+        max_episode_steps=max_episode_steps,
+        seed=seed,
+        noop_max=30,
+        terminal_on_life_loss=False,
+        sticky_action=False,
+        clip_reward=False,
+        reward_type=reward_type,
+    )
+
 def main(argv):
     """Tests Agent57 agent."""
     del argv
     runtime_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    logging.info('Runtime device: %s', runtime_device)
     np.random.seed(FLAGS.seed)
     torch.manual_seed(FLAGS.seed)
     if torch.backends.cudnn.enabled:
@@ -73,22 +92,28 @@ def main(argv):
 
     random_state = np.random.RandomState(FLAGS.seed)  # pylint: disable=no-member
 
-    # Create evaluation environment
-    eval_env = gym_env.create_atari_environment(
-        env_name=FLAGS.environment_name,
-        frame_height=FLAGS.environment_height,
-        frame_width=FLAGS.environment_width,
-        frame_skip=FLAGS.environment_frame_skip,
-        frame_stack=FLAGS.environment_frame_stack,
-        max_episode_steps=FLAGS.max_episode_steps,
-        seed=random_state.randint(1, 2**10),
-        noop_max=30,
-        terminal_on_life_loss=False,
-        sticky_action=False,
-        clip_reward=False,
-    )
-    state_dim = (FLAGS.environment_frame_stack, FLAGS.environment_height, FLAGS.environment_width)
-    action_dim = eval_env.action_space.n
+    random_state = np.random.RandomState(FLAGS.seed)  # pylint: disable=no-member
+
+    # Create evaluation environment, like R2D2, we disable terminate-on-life-loss and clip reward.
+    
+
+    eval_env = environment_builder(            
+            environment_name=FLAGS.environment_name,
+            environment_height=FLAGS.environment_height,
+            environment_width=FLAGS.environment_width,
+            environment_frame_skip=FLAGS.environment_frame_skip,
+            environment_frame_stack=FLAGS.environment_frame_stack,
+            max_episode_steps=FLAGS.max_episode_steps,
+            seed=random_state.randint(1, 2**10),
+            reward_type=FLAGS.reward,)
+
+    # eval_env.reset()
+    # exit()
+
+    state_dim = eval_env.obs_shape #eval_env.observation_space.shape
+    action_dim = eval_env.num_actions #eval_env.action_space.n
+    # state_dim = (FLAGS.environment_frame_stack, FLAGS.environment_height, FLAGS.environment_width)
+    # action_dim = eval_env.action_space.n
     network = Agent57Conv2dNet(state_dim=state_dim, action_dim=action_dim, num_policies=FLAGS.num_policies)
     rnd_target_network = RndConvNet(state_dim=state_dim)
     rnd_predictor_network = RndConvNet(state_dim=state_dim)
@@ -137,6 +162,7 @@ def main(argv):
         eval_env=eval_env,
         use_tensorboard=FLAGS.use_tensorboard,
         recording_video_dir=FLAGS.recording_video_dir,
+        root_dir = FLAGS.root_dir
     )
 
 
diff --git a/deep_rl_zoo/agent57/run_retro.py b/deep_rl_zoo/agent57/run_retro.py
new file mode 100644
index 0000000..7d2fcb7
--- /dev/null
+++ b/deep_rl_zoo/agent57/run_retro.py
@@ -0,0 +1,412 @@
+# Copyright 2022 The Deep RL Zoo Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""
+From the paper "Agent57: Outperforming the Atari Human Benchmark"
+https://arxiv.org/pdf/2003.13350.
+"""
+
+from functools import partial
+from absl import app
+from absl import flags
+from absl import logging
+import os
+
+os.environ['OMP_NUM_THREADS'] = '1'
+os.environ['MKL_NUM_THREADS'] = '1'
+
+import multiprocessing
+import numpy as np
+import torch
+import copy
+
+# pylint: disable=import-error
+from deep_rl_zoo.networks.value import Agent57Conv2dNet, Agent57NetworkInputs
+from deep_rl_zoo.networks.curiosity import NGURndConvNet, NguEmbeddingConvNet
+from deep_rl_zoo.agent57 import agent
+from deep_rl_zoo.checkpoint import PyTorchCheckpoint
+from deep_rl_zoo import main_loop_gymnasium as main_loop
+from deep_rl_zoo import gym_env_gymnasium as gym_env
+from deep_rl_zoo import greedy_actors
+from deep_rl_zoo import replay as replay_lib
+
+
+FLAGS = flags.FLAGS
+flags.DEFINE_string(
+    'environment_name', 'Pong', 'Atari name without NoFrameskip and version, like Breakout, Pong, Seaquest.'
+)  # MontezumaRevenge, Pitfall, Solaris, Skiing
+flags.DEFINE_integer('environment_height', 84, 'Environment frame screen height.')
+flags.DEFINE_integer('environment_width', 84, 'Environment frame screen width.')
+flags.DEFINE_integer('environment_frame_skip', 4, 'Number of frames to skip.')
+flags.DEFINE_integer('environment_frame_stack', 1, 'Number of frames to stack.')
+flags.DEFINE_bool('compress_state', True, 'Compress state images when store in experience replay.')
+flags.DEFINE_integer('num_actors', 8, 'Number of actor processes to run in parallel.')
+flags.DEFINE_integer('replay_capacity', 20000, 'Maximum replay size (in number of unrolls stored).')  # watch for out of RAM
+flags.DEFINE_integer(
+    'min_replay_size', 1000, 'Minimum replay size before learning starts (in number of unrolls stored).'
+)  # 6250
+flags.DEFINE_bool('clip_grad', True, 'Clip gradients, default on.')
+flags.DEFINE_float('max_grad_norm', 40.0, 'Max gradients norm when do gradients clip.')
+
+flags.DEFINE_float('learning_rate', 0.0001, 'Learning rate for adam.')
+flags.DEFINE_float(
+    'int_learning_rate', 0.0005, 'Intrinsic learning rate for adam, this is for embedding and RND predictor networks.'
+)
+flags.DEFINE_float('ext_discount', 0.997, 'Extrinsic reward discount rate.')
+flags.DEFINE_float('int_discount', 0.99, 'Intrinsic reward discount rate.')
+flags.DEFINE_float('adam_eps', 0.0001, 'Epsilon for adam.')
+flags.DEFINE_integer('unroll_length', 80, 'Sequence of transitions to unroll before add to replay.')
+flags.DEFINE_integer(
+    'burn_in',
+    40,
+    'Sequence of transitions used to pass RNN before actual learning.'
+    'The effective length of unrolls will be burn_in + unroll_length, '
+    'two consecutive unrolls will overlap on burn_in steps.',
+)
+flags.DEFINE_integer('batch_size', 32, 'Batch size for learning.')
+
+flags.DEFINE_float('policy_beta', 0.3, 'Scalar for the intrinsic reward scale.')
+flags.DEFINE_integer('num_policies', 32, 'Number of directed policies to learn, scaled by intrinsic reward scale beta.')
+flags.DEFINE_integer('ucb_window_size', 90, 'Sliding window size of the UCB algorithm.')
+flags.DEFINE_float('ucb_beta', 1.0, 'Beta for the UCB algorithm.')
+flags.DEFINE_float('ucb_epsilon', 0.5, 'Exploration epsilon for the UCB algorithm.')
+
+flags.DEFINE_integer('episodic_memory_capacity', 3000, 'Maximum size of episodic memory.')  # 30000
+flags.DEFINE_bool(
+    'reset_episodic_memory',
+    True,
+    'Reset the episodic_memory on every episode, only applicable to actors, default on.'
+    'From NGU Paper on MontezumaRevenge, Instead of resetting the memory after every episode, we do it after a small number of '
+    'consecutive episodes, which we call a meta-episode. This structure plays an important role when the'
+    'agent faces irreversible choices.',
+)
+flags.DEFINE_integer('num_neighbors', 10, 'Number of K-nearest neighbors.')
+flags.DEFINE_float('kernel_epsilon', 0.0001, 'K-nearest neighbors kernel epsilon.')
+flags.DEFINE_float('cluster_distance', 0.008, 'K-nearest neighbors custer distance.')
+flags.DEFINE_float('max_similarity', 8.0, 'K-nearest neighbors custer distance.')
+
+flags.DEFINE_float('retrace_lambda', 0.95, 'Lambda coefficient for retrace.')
+flags.DEFINE_bool('transformed_retrace', True, 'Transformed retrace loss, default on.')
+
+flags.DEFINE_float('priority_exponent', 0.9, 'Priority exponent used in prioritized replay.')
+flags.DEFINE_float('importance_sampling_exponent', 0.6, 'Importance sampling exponent value.')
+flags.DEFINE_bool('normalize_weights', True, 'Normalize sampling weights in prioritized replay.')
+flags.DEFINE_float('priority_eta', 0.9, 'Priority eta to mix the max and mean absolute TD errors.')
+
+flags.DEFINE_integer('num_iterations', 100, 'Number of iterations to run.')
+flags.DEFINE_integer(
+    'num_train_steps', int(5e5), 'Number of training steps (environment steps or frames) to run per iteration, per actor.'
+)
+flags.DEFINE_integer(
+    'num_eval_steps', int(2e4), 'Number of evaluation steps (environment steps or frames) to run per iteration.'
+)
+flags.DEFINE_integer('max_episode_steps', 108000, 'Maximum steps (before frame skip) per episode.')
+flags.DEFINE_integer(
+    'target_net_update_interval',
+    1500,
+    'The interval (meassured in Q network updates) to update target Q networks.',
+)
+flags.DEFINE_integer('actor_update_interval', 100, 'The frequency (measured in actor steps) to update actor local Q network.')
+flags.DEFINE_float('eval_exploration_epsilon', 0.01, 'Fixed exploration rate in e-greedy policy for evaluation.')
+flags.DEFINE_integer('seed', 1, 'Runtime seed.')
+flags.DEFINE_bool('use_tensorboard', True, 'Use Tensorboard to monitor statistics, default on.')
+flags.DEFINE_bool('actors_on_gpu', True, 'Run actors on GPU, default on.')
+flags.DEFINE_integer(
+    'debug_screenshots_interval',
+    0,
+    'Take screenshots every N episodes and log to Tensorboard, default 0 no screenshots.',
+)
+flags.DEFINE_string('tag', '', 'Add tag to Tensorboard log file.')
+flags.DEFINE_string('results_csv_path', './logs/agent57_retro_results.csv', 'Path for CSV log file.')
+flags.DEFINE_string('checkpoint_dir', './checkpoints', 'Path for checkpoint directory.')
+flags.DEFINE_string('root_dir', '/scratch/nedko_savov/projects/ivg/external/open-genie/agent57/runs', 'Model name.')
+flags.DEFINE_string('load_checkpoint_file', '', 'Load checkpoint file.')
+flags.DEFINE_string('model_name', '000_debug', 'Model name.')
+flags.DEFINE_string('reward', 'combined', 'Reward type.')
+
+flags.register_validator('environment_frame_stack', lambda x: x == 1)
+
+def environment_builder(environment_name, environment_height, environment_width, environment_frame_skip, environment_frame_stack, max_episode_steps, seed, reward_type):
+    return gym_env.create_retro_environment(
+        env_name=environment_name,
+        frame_height=environment_height,
+        frame_width=environment_width,
+        frame_skip=environment_frame_skip,
+        frame_stack=environment_frame_stack,
+        max_episode_steps=max_episode_steps,
+        seed=seed,
+        noop_max=30,
+        terminal_on_life_loss=False,
+        sticky_action=False,
+        clip_reward=False,
+        reward_type=reward_type,
+    )
+def main(argv):
+    """Trains Agent57 agent on Atari."""
+    del argv
+    runtime_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    logging.info(f'Runs Agent57 agent on {runtime_device}')
+    np.random.seed(FLAGS.seed)
+    torch.manual_seed(FLAGS.seed)
+    if torch.backends.cudnn.enabled:
+        torch.backends.cudnn.benchmark = False
+        torch.backends.cudnn.deterministic = True
+
+    random_state = np.random.RandomState(FLAGS.seed)  # pylint: disable=no-member
+
+    # Create evaluation environment, like R2D2, we disable terminate-on-life-loss and clip reward.
+    
+
+    eval_env = environment_builder(            
+            environment_name=FLAGS.environment_name,
+            environment_height=FLAGS.environment_height,
+            environment_width=FLAGS.environment_width,
+            environment_frame_skip=FLAGS.environment_frame_skip,
+            environment_frame_stack=FLAGS.environment_frame_stack,
+            max_episode_steps=FLAGS.max_episode_steps,
+            seed=random_state.randint(1, 2**10),
+            reward_type=FLAGS.reward)
+
+    state_dim = eval_env.obs_shape #eval_env.observation_space.shape
+
+    action_dim = eval_env.num_actions #eval_env.action_space.n
+
+    logging.info('Environment: %s', FLAGS.environment_name)
+    logging.info('Action spec: %s', action_dim)
+    logging.info('Observation spec: %s', state_dim)
+
+    # Test environment and state shape.
+
+    obs = eval_env.reset()
+    print(obs.shape, (FLAGS.environment_frame_stack, FLAGS.environment_height, FLAGS.environment_width))
+    assert isinstance(obs, np.ndarray)
+    assert obs.shape == (FLAGS.environment_frame_stack, FLAGS.environment_height, FLAGS.environment_width)
+
+    # Create extrinsic and intrinsic reward Q networks for learner to optimize.
+    network = Agent57Conv2dNet(state_dim=state_dim, action_dim=action_dim, num_policies=FLAGS.num_policies)
+    optimizer = torch.optim.Adam(network.parameters(), lr=FLAGS.learning_rate, eps=FLAGS.adam_eps)
+
+    # Create RND target and predictor networks.
+    rnd_target_network = NGURndConvNet(state_dim=state_dim, is_target=True)
+    rnd_predictor_network = NGURndConvNet(state_dim=state_dim, is_target=False)
+
+    # Create embedding networks.
+    embedding_network = NguEmbeddingConvNet(state_dim=state_dim, action_dim=action_dim)
+
+    # Second Adam optimizer for embedding and RND predictor networks.
+    intrinsic_optimizer = torch.optim.Adam(
+        list(embedding_network.parameters()) + list(rnd_predictor_network.parameters()),
+        lr=FLAGS.int_learning_rate,
+        eps=FLAGS.adam_eps,
+    )
+
+    ext_state, int_state = network.get_initial_hidden_state(1)
+
+    # Test network output.
+    x = Agent57NetworkInputs(
+        s_t=torch.from_numpy(obs[None, None, ...]).float(),
+        a_tm1=torch.zeros(1, 1).long(),
+        ext_r_t=torch.zeros(1, 1).float(),
+        int_r_t=torch.zeros(1, 1).float(),
+        policy_index=torch.zeros(1, 1).long(),
+        ext_hidden_s=ext_state,
+        int_hidden_s=int_state,
+    )
+    network_output = network(x)
+    assert network_output.ext_q_values.shape == (1, 1, action_dim)
+    assert network_output.int_q_values.shape == (1, 1, action_dim)
+    assert len(network_output.ext_hidden_s) == 2
+    assert len(network_output.int_hidden_s) == 2
+
+    # Create prioritized transition replay, no importance_sampling_exponent decay
+    importance_sampling_exponent = FLAGS.importance_sampling_exponent
+
+    def importance_sampling_exponent_schedule(x):
+        return importance_sampling_exponent
+
+    # Create transition replay
+    if FLAGS.compress_state:
+
+        def encoder(transition):
+            return transition._replace(
+                s_t=replay_lib.compress_array(transition.s_t),
+            )
+
+        def decoder(transition):
+            return transition._replace(
+                s_t=replay_lib.uncompress_array(transition.s_t),
+            )
+
+    else:
+        encoder = None
+        decoder = None
+
+    replay = replay_lib.PrioritizedReplay(
+        capacity=FLAGS.replay_capacity,
+        structure=agent.TransitionStructure,
+        priority_exponent=FLAGS.priority_exponent,
+        importance_sampling_exponent=importance_sampling_exponent_schedule,
+        normalize_weights=FLAGS.normalize_weights,
+        random_state=random_state,
+        time_major=True,
+        encoder=encoder,
+        decoder=decoder,
+    )
+
+    # Create queue to shared transitions between actors and learner
+    data_queue = multiprocessing.Queue(maxsize=FLAGS.num_actors * 2)
+    # Create shared objects so all actor processes can access them
+    manager = multiprocessing.Manager()
+
+    # Store copy of latest parameters of the neural network in a shared dictionary, so actors can later access it
+    shared_params = manager.dict(
+        {
+            'network': None,
+            'embedding_network': None,
+            'rnd_predictor_network': None,
+        }
+    )
+
+    # Create Agent57 learner instance
+    learner_agent = agent.Learner(
+        network=network,
+        optimizer=optimizer,
+        embedding_network=embedding_network,
+        rnd_target_network=rnd_target_network,
+        rnd_predictor_network=rnd_predictor_network,
+        intrinsic_optimizer=intrinsic_optimizer,
+        replay=replay,
+        min_replay_size=FLAGS.min_replay_size,
+        target_net_update_interval=FLAGS.target_net_update_interval,
+        unroll_length=FLAGS.unroll_length,
+        burn_in=FLAGS.burn_in,
+        retrace_lambda=FLAGS.retrace_lambda,
+        transformed_retrace=FLAGS.transformed_retrace,
+        priority_eta=FLAGS.priority_eta,
+        batch_size=FLAGS.batch_size,
+        clip_grad=FLAGS.clip_grad,
+        max_grad_norm=FLAGS.max_grad_norm,
+        device=runtime_device,
+        shared_params=shared_params,
+    )
+
+    # Create actor environments, actor instances.
+    actor_envs = [partial(environment_builder,        
+            environment_name=FLAGS.environment_name,
+            environment_height=FLAGS.environment_height,
+            environment_width=FLAGS.environment_width,
+            environment_frame_skip=FLAGS.environment_frame_skip,
+            environment_frame_stack=FLAGS.environment_frame_stack,
+            max_episode_steps=FLAGS.max_episode_steps,
+            seed=random_state.randint(1, 2**10),
+            reward_type=FLAGS.reward) for _ in range(FLAGS.num_actors)
+            
+            ]
+
+    actor_devices = ['cpu'] * FLAGS.num_actors
+    print(torch.cuda.is_available(), FLAGS.actors_on_gpu)
+    # exit()
+    # Evenly distribute the actors to all available GPUs
+    if torch.cuda.is_available() and FLAGS.actors_on_gpu:
+        num_gpus = torch.cuda.device_count()
+        actor_devices = [torch.device(f'cuda:{i % num_gpus}') for i in range(FLAGS.num_actors)]
+
+    # Each actor has it's own embedding and RND predictor networks,
+    # because we don't want to update these network parameters in the middle of an episode,
+    # it will only update these networks at the beginning of an episode.
+    actors = [
+        agent.Actor(
+            rank=i,
+            data_queue=data_queue,
+            network=copy.deepcopy(network),
+            rnd_target_network=copy.deepcopy(rnd_target_network),
+            rnd_predictor_network=copy.deepcopy(rnd_predictor_network),
+            embedding_network=copy.deepcopy(embedding_network),
+            random_state=np.random.RandomState(FLAGS.seed + int(i)),  # pylint: disable=no-member
+            ext_discount=FLAGS.ext_discount,
+            int_discount=FLAGS.int_discount,
+            num_policies=FLAGS.num_policies,
+            policy_beta=FLAGS.policy_beta,
+            ucb_window_size=FLAGS.ucb_window_size,
+            ucb_beta=FLAGS.ucb_beta,
+            ucb_epsilon=FLAGS.ucb_epsilon,
+            episodic_memory_capacity=FLAGS.episodic_memory_capacity,
+            reset_episodic_memory=FLAGS.reset_episodic_memory,
+            num_neighbors=FLAGS.num_neighbors,
+            kernel_epsilon=FLAGS.kernel_epsilon,
+            cluster_distance=FLAGS.cluster_distance,
+            max_similarity=FLAGS.max_similarity,
+            num_actors=FLAGS.num_actors,
+            action_dim=action_dim,
+            unroll_length=FLAGS.unroll_length,
+            burn_in=FLAGS.burn_in,
+            actor_update_interval=FLAGS.actor_update_interval,
+            device=actor_devices[i],
+            shared_params=shared_params,
+        )
+        for i in range(FLAGS.num_actors)
+    ]
+
+    # Create evaluation agent instance
+    eval_agent = greedy_actors.Agent57EpsilonGreedyActor(
+        network=network,
+        embedding_network=embedding_network,
+        rnd_target_network=rnd_target_network,
+        rnd_predictor_network=rnd_predictor_network,
+        exploration_epsilon=FLAGS.eval_exploration_epsilon,
+        episodic_memory_capacity=FLAGS.episodic_memory_capacity,
+        num_neighbors=FLAGS.num_neighbors,
+        kernel_epsilon=FLAGS.kernel_epsilon,
+        cluster_distance=FLAGS.cluster_distance,
+        max_similarity=FLAGS.max_similarity,
+        random_state=random_state,
+        device=runtime_device,
+    )
+
+    # Setup checkpoint.
+    checkpoint = PyTorchCheckpoint(
+        environment_name=FLAGS.environment_name, agent_name='Agent57', save_dir=f"{FLAGS.checkpoint_dir}/{FLAGS.model_name}"
+    )
+    checkpoint.register_pair(('network', network))
+    checkpoint.register_pair(('rnd_target_network', rnd_target_network))
+    checkpoint.register_pair(('rnd_predictor_network', rnd_predictor_network))
+    checkpoint.register_pair(('embedding_network', embedding_network))
+
+    if FLAGS.load_checkpoint_file:
+        checkpoint.restore(FLAGS.load_checkpoint_file)
+    
+    # Run parallel training N iterations.
+    main_loop.run_parallel_training_iterations(
+        num_iterations=FLAGS.num_iterations,
+        num_train_steps=FLAGS.num_train_steps,
+        num_eval_steps=FLAGS.num_eval_steps,
+        learner_agent=learner_agent,
+        eval_agent=eval_agent,
+        eval_env=eval_env,
+        actors=actors,
+        actor_envs=actor_envs,
+        data_queue=data_queue,
+        checkpoint=checkpoint,
+        csv_file=f"{FLAGS.checkpoint_dir}/{FLAGS.model_name}/results.csv",
+        use_tensorboard=FLAGS.use_tensorboard,
+        tag=FLAGS.tag,
+        debug_screenshots_interval=FLAGS.debug_screenshots_interval,
+        root_dir=f"{FLAGS.root_dir}/{FLAGS.model_name}",
+    )
+
+
+if __name__ == '__main__':
+    # Set multiprocessing start mode
+    multiprocessing.set_start_method('spawn')
+    app.run(main)
diff --git a/deep_rl_zoo/checkpoint.py b/deep_rl_zoo/checkpoint.py
index 483a47c..21f5258 100644
--- a/deep_rl_zoo/checkpoint.py
+++ b/deep_rl_zoo/checkpoint.py
@@ -125,12 +125,12 @@ class PyTorchCheckpoint:
         loaded_state = torch.load(file_to_restore, map_location=torch.device('cpu'))
 
         # Needs to match environment_name and agent name
-        if loaded_state['environment_name'] != self.state.environment_name:
-            err_msg = f'environment_name "{loaded_state["environment_name"]}" and "{self.state.environment_name}" mismatch.'
-            raise RuntimeError(err_msg)
-        if 'agent_name' in loaded_state and loaded_state['agent_name'] != self.state.agent_name:
-            err_msg = f'agent_name "{loaded_state["agent_name"]}" and "{self.state.agent_name}" mismatch.'
-            raise RuntimeError(err_msg)
+        # if loaded_state['environment_name'] != self.state.environment_name:
+        #     err_msg = f'environment_name "{loaded_state["environment_name"]}" and "{self.state.environment_name}" mismatch.'
+        #     raise RuntimeError(err_msg)
+        # if 'agent_name' in loaded_state and loaded_state['agent_name'] != self.state.agent_name:
+        #     err_msg = f'agent_name "{loaded_state["agent_name"]}" and "{self.state.agent_name}" mismatch.'
+        #     raise RuntimeError(err_msg)
 
         # Ready to restore the states.
         loaded_keys = [k for k in loaded_state.keys()]
@@ -141,7 +141,7 @@ class PyTorchCheckpoint:
                 continue
 
             if self._is_torch_model(item):
-                self.state[key].load_state_dict(loaded_state[key])
+                self.state[key].load_state_dict(loaded_state[key], strict=False)
             else:
                 self.state[key] = loaded_state[key]
 
diff --git a/deep_rl_zoo/done_tracker.py b/deep_rl_zoo/done_tracker.py
new file mode 100644
index 0000000..41ff762
--- /dev/null
+++ b/deep_rl_zoo/done_tracker.py
@@ -0,0 +1,27 @@
+import numpy as np
+
+
+class DoneTrackerEnv:
+    def __init__(self, num_envs: int) -> None:
+        """Monitor env dones: 0 when not done, 1 when done, 2 when already done."""
+        self.num_envs = num_envs
+        self.done_tracker = None
+        self.reset_done_tracker()
+
+    def reset_done_tracker(self) -> None:
+        self.done_tracker = np.zeros(self.num_envs, dtype=np.uint8)
+
+    def update_done_tracker(self, done: np.ndarray) -> None:
+        self.done_tracker = np.clip(2 * self.done_tracker + done, 0, 2)
+
+    @property
+    def num_envs_done(self) -> int:
+        return (self.done_tracker > 0).sum()
+
+    @property
+    def mask_dones(self) -> np.ndarray:
+        return np.logical_not(self.done_tracker)
+
+    @property
+    def mask_new_dones(self) -> np.ndarray:
+        return np.logical_not(self.done_tracker[self.done_tracker <= 1])
diff --git a/deep_rl_zoo/gym_env.py b/deep_rl_zoo/gym_env.py
index 1ad9dcd..1e22d0e 100644
--- a/deep_rl_zoo/gym_env.py
+++ b/deep_rl_zoo/gym_env.py
@@ -27,18 +27,25 @@
 # ==============================================================================
 """gym environment processing components."""
 
+from copy import deepcopy
+from functools import partial
 import os
 import datetime
+import random
 import numpy as np
 import cv2
 import logging
-import gym
-from gym.spaces import Box
+# import gym
+# from gym.spaces import Box
+import gymnasium
+import gymnasium as gym
+from gymnasium.spaces import Box
 from collections import deque
 from pathlib import Path
-
+import retro
 # pylint: disable=import-error
 import deep_rl_zoo.types as types_lib
+from gymnasium.envs.registration import EnvSpec
 
 # A simple list of classic env names.
 CLASSIC_ENV_NAMES = ['CartPole-v1', 'LunarLander-v2', 'MountainCar-v0', 'Acrobot-v1']
@@ -65,7 +72,8 @@ class NoopReset(gym.Wrapper):
         self.noop_max = noop_max
         self.override_num_noops = None
         self.noop_action = 0
-        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'
+        # print(env.unwrapped.get_action_meaning([0]))
+        # assert env.unwrapped.get_action_meaning([0]) == 'NOOP'
 
     def reset(self, **kwargs):
         """Do no-op action for a number of steps in [1, noop_max]."""
@@ -77,7 +85,8 @@ class NoopReset(gym.Wrapper):
         assert noops > 0
         obs = None
         for _ in range(noops):
-            obs, _, done, _ = self.env.step(self.noop_action)
+            obs, _, terminated, truncated, _ = self.env.step(self.noop_action)
+            done = terminated or truncated
             if done:
                 obs = self.env.reset(**kwargs)
         return obs
@@ -418,6 +427,351 @@ class RecordRawReward(gym.Wrapper):
         return obs, reward, done, info
 
 
+class OneHotEncoding(gym.Space):
+    """
+    
+    Based on: https://stackoverflow.com/questions/54022606/openai-gym-how-to-create-one-hot-observation-space
+    {0,...,1,...,0}
+
+    Example usage:
+    self.observation_space = OneHotEncoding(size=4)
+    """
+    def __init__(self, size=None):
+        assert isinstance(size, int) and size > 0
+        self.size = size
+        self.n=size
+        gym.Space.__init__(self, (self.size,), np.int64)
+
+    def sample(self):
+        one_hot_vector = np.array([0] * self.size) #np.zeros(self.size)
+        one_hot_vector[np.random.randint(self.size)] = 1
+        return one_hot_vector
+
+    def contains(self, x):
+        if isinstance(x, (list, tuple, np.ndarray)):
+            number_of_zeros = list(x).count(0)
+            number_of_ones = list(x).count(1)
+            return (number_of_zeros == (self.size - 1)) and (number_of_ones == 1)
+        else:
+            return False
+
+    def __repr__(self):
+        return "OneHotEncoding(%d)" % self.size
+
+    def __eq__(self, other):
+        return self.size == other.size
+
+
+class Discretizer(gym.ActionWrapper):
+    """
+    Wrap a gym environment and make it use one hot discrete actions.
+    based on https://gist.github.com/christopherhesse/8e5c63c3b0007f4c3e333c6a158872cf
+    Args:
+        combos: ordered list of lists of valid button combinations
+    """
+    REMAPPER = {
+        "atari2600": {
+            "A": "BUTTON",
+            "B": "BUTTON",
+            "ACTION_JUMP": "BUTTON",
+            "ACTION_PRIMARY": "BUTTON",
+            "ACTION_SECONDARY": "BUTTON",
+        },
+        "nes": {
+            "ACTION_JUMP": "A",
+            "ACTION_PRIMARY": "A",
+            "ACTION_SECONDARY": "B"
+        },
+        "snes": {
+            "ACTION_JUMP": "B",
+            "ACTION_PRIMARY": "B",
+            "ACTION_SECONDARY": "Y"
+        },
+        "genesis": {
+            "ACTION_JUMP": "C",
+            "ACTION_PRIMARY": "C",
+            "ACTION_SECONDARY": "B"
+        },
+        "sms": {
+            "ACTION_JUMP": "A",
+            "ACTION_PRIMARY": "A",
+            "ACTION_SECONDARY": "B"
+        },
+        "gameboy": {
+            "ACTION_JUMP": "A",
+            "ACTION_PRIMARY": "A",
+            "ACTION_SECONDARY": "B"
+        },
+        "32x": {
+            "ACTION_JUMP": "B",
+            "ACTION_PRIMARY": "B",
+            "ACTION_SECONDARY": "Y"
+        },
+    }
+
+    def __init__(self, env, platform, combos=None):
+        super().__init__(env)
+        assert isinstance(env.action_space, gym.spaces.MultiBinary)
+        buttons = env.unwrapped.buttons
+        # if len(buttons) == 0: 
+        #     buttons = ['B', 'Y', 'SELECT', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'A', 'X', 'L', 'R']
+        self._decode_discrete_action = []
+        self.combos = combos
+
+        if self.combos is None:
+            self.combos = buttons 
+
+        self.combos = [combo if isinstance(combo, list) else [combo] for combo in self.combos]
+
+        try:
+            for combo in self.combos:
+                arr = np.array([0] * env.action_space.n)
+                for button in combo:
+                    button = self.REMAPPER[platform.lower()].get(button, button)
+                    if button.upper() != "NOOP":
+                        arr[buttons.index(button)] = 1
+                    # arr[button] = 1
+                    
+                self._decode_discrete_action.append(arr)
+        except:
+            raise ValueError(f"Invalid action combination {buttons}")
+        self.action_space = OneHotEncoding(len(self._decode_discrete_action))
+
+    def action(self, act):
+        
+        act = np.argmax(act)
+        return self._decode_discrete_action[act].copy()
+
+
+class MultiEnvWrapper:
+    def __init__(self, fn_env_create, games):
+        random.seed()
+        np.random.seed()
+
+        game = random.sample(games, 1)[0]
+        print(game)
+        self.env = fn_env_create(game=game)
+        self.games = games
+        self.fn_env_create = fn_env_create
+
+    def __getattr__(self, name):
+        return getattr(self.env, name)
+
+    def reset(self, **kwargs):
+        self.env.close()
+        game = random.sample(self.games, 1)[0]
+        print(game)
+        self.env = self.fn_env_create(game=game)
+        return self.env.reset(**kwargs)
+
+    def step(self, *args, **kwargs):
+        return self.env.step(*args, **kwargs)
+
+class TimeLimit(gym.Wrapper, gym.utils.RecordConstructorArgs):
+    """This wrapper will issue a `truncated` signal if a maximum number of timesteps is exceeded.
+
+    If a truncation is not defined inside the environment itself, this is the only place that the truncation signal is issued.
+    Critically, this is different from the `terminated` signal that originates from the underlying environment as part of the MDP.
+
+    Example:
+       >>> import gymnasium as gym
+       >>> from gymnasium.wrappers import TimeLimit
+       >>> env = gym.make("CartPole-v1")
+       >>> env = TimeLimit(env, max_episode_steps=1000)
+    """
+
+    def __init__(
+        self,
+        env: gym.Env,
+        max_episode_steps: int,
+    ):
+        """Initializes the :class:`TimeLimit` wrapper with an environment and the number of steps after which truncation will occur.
+
+        Args:
+            env: The environment to apply the wrapper
+            max_episode_steps: An optional max episode steps (if ``None``, ``env.spec.max_episode_steps`` is used)
+        """
+        gym.utils.RecordConstructorArgs.__init__(
+            self, max_episode_steps=max_episode_steps
+        )
+        gym.Wrapper.__init__(self, env)
+
+        self._max_episode_steps = max_episode_steps
+        self._elapsed_steps = None
+
+    def step(self, action):
+        """Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.
+
+        Args:
+            action: The environment step action
+
+        Returns:
+            The environment step ``(observation, reward, terminated, truncated, info)`` with `truncated=True`
+            if the number of steps elapsed >= max episode steps
+
+        """
+        observation, reward, terminated, truncated, info = self.env.step(action)
+        self._elapsed_steps += 1
+
+        if self._elapsed_steps >= self._max_episode_steps:
+            truncated = True
+
+        return observation, reward, terminated, truncated, info
+
+    def reset(self, **kwargs):
+        """Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.
+
+        Args:
+            **kwargs: The kwargs to reset the environment with
+
+        Returns:
+            The reset environment
+        """
+        self._elapsed_steps = 0
+        return self.env.reset(**kwargs)
+
+    @property
+    def spec(self) -> EnvSpec | None:
+        """Modifies the environment spec to include the `max_episode_steps=self._max_episode_steps`."""
+        if self._cached_spec is not None:
+            return self._cached_spec
+
+        env_spec = self.env.spec
+        if env_spec is not None:
+            env_spec = deepcopy(env_spec)
+            env_spec.max_episode_steps = self._max_episode_steps
+
+        self._cached_spec = env_spec
+        return env_spec
+
+def make_retro(*, game, state=None, max_episode_steps=None, skip_frames=1, render_mode="human", use_discretization=True, valid_action_combos=None, **kwargs):
+    if state is None:
+        state = retro.State.DEFAULT
+    if skip_frames < 1:
+        raise ValueError(f"skip_frames must be at least 1, got {skip_frames}")
+
+    env = retro.make(game, state, render_mode=render_mode, **kwargs)
+    # env = StochasticFrameSkip(env, n=skip_frames, stickprob=0, want_render=True)
+    if max_episode_steps is not None:
+        env = TimeLimit(env, max_episode_steps=max_episode_steps)
+    
+    if use_discretization:
+        platform = game.split("-")[1]
+        env = Discretizer(env, platform, combos=valid_action_combos)
+    return env
+
+def create_retro(game, max_episode_steps, frame_skip, num_envs, transform=None):
+    # env_fn = partial(instantiate, config=cfg_env)
+    games = [game]#["SuperMarioBros-Nes"] #["AdventureIslandII-Nes"] #get_game_list()
+    max_episode_steps = max_episode_steps
+    frame_skip = frame_skip
+    valid_action_combos = ["UP", "DOWN", "RIGHT", "LEFT", "ACTION_JUMP"]
+
+    def make_retro_multi(games):
+
+        env_fn = partial(
+            make_retro,
+            render_mode="rgb_array",
+            valid_action_combos=valid_action_combos,
+            skip_frames=frame_skip,
+            max_episode_steps=max_episode_steps
+        )
+        env = MultiEnvWrapper(env_fn, games)
+        return env
+    
+    env = make_retro_multi(games=games)
+
+    # env = MultiProcessEnv(fn_make_env, num_envs, should_wait_num_envs_ratio=0.5, transform=transform)
+    return env
+
+
+def create_retro_environment(
+    env_name: str,
+    seed: int = 1,
+    frame_skip: int = 4,
+    frame_stack: int = 4,
+    frame_height: int = 84,
+    frame_width: int = 84,
+    noop_max: int = 30,
+    max_episode_steps: int = 108000,
+    obscure_epsilon: float = 0.0,
+    terminal_on_life_loss: bool = False,
+    clip_reward: bool = True,
+    sticky_action: bool = True,
+    scale_obs: bool = False,
+    channel_first: bool = True,
+) -> gymnasium.Env:
+    """
+    Process gym env for Atari games according to the Nature DQN paper.
+
+    Args:
+        env_name: the environment name without 'NoFrameskip' and version.
+        seed: seed the runtime.
+        frame_skip: the frequency at which the agent experiences the game,
+                the environment will also repeat action.
+        frame_stack: stack n last frames.
+        frame_height: height of the resized frame.
+        frame_width: width of the resized frame.
+        noop_max: maximum number of no-ops to apply at the beginning
+                of each episode to reduce determinism. These no-ops are applied at a
+                low-level, before frame skipping.
+        max_episode_steps: maximum steps for an episode.
+        obscure_epsilon: with epsilon probability [0.0, 1.0), obscure the state to make it POMDP.
+        terminal_on_life_loss: if True, mark end of game when loss a life, default off.
+        clip_reward: clip reward in the range of [-1, 1], default on.
+        sticky_action: if True, randomly re-use last action with 0.25 probability, default on.
+        scale_obs: scale the frame by divide 255, turn this on may require 4-5x more RAM when using experience replay, default off.
+        channel_first: if True, change observation image from shape [H, W, C] to in the range [C, H, W], this is for PyTorch only, default on.
+
+    Returns:
+        preprocessed gym.Env for Atari games.
+    """
+    if 'NoFrameskip' in env_name:
+        raise ValueError(f'Environment name should not include NoFrameskip, got {env_name}')
+
+    env = create_retro(env_name, None, 1, 1) #gym.make(f'{env_name}NoFrameskip-v4')
+    # unwrap(env).seed(seed)
+
+
+    # Change TimeLimit wrapper to 108,000 steps (30 min) as default in the
+    # literature instead of OpenAI Gym's default of 100,000 steps.
+    env = gymnasium.wrappers.TimeLimit(env.env, max_episode_steps=None if max_episode_steps <= 0 else max_episode_steps)
+
+    if noop_max > 0:
+        env = NoopReset(env, noop_max=noop_max)
+    if sticky_action:
+        env = StickyAction(env)
+    if frame_skip > 0:
+        env = MaxAndSkip(env, skip=frame_skip)
+
+    # Obscure observation with obscure_epsilon probability
+    if obscure_epsilon > 0.0:
+        env = ObscureObservation(env, obscure_epsilon)
+    if terminal_on_life_loss:
+        env = LifeLoss(env)
+
+    env = ResizeAndGrayscaleFrame(env, width=frame_width, height=frame_height)
+
+    if scale_obs:
+        env = ScaleFrame(env)
+
+    if clip_reward:
+        env = RecordRawReward(env)
+        env = ClipRewardWithBound(env, 1.0)
+
+    if frame_stack > 1:
+        env = FrameStack(env, frame_stack)
+    if channel_first:
+        env = ObservationChannelFirst(env, scale_obs)
+    else:
+        # This is required as LazeFrame object is not numpy.array.
+        env = ObservationToNumpy(env)
+
+    if 'Montezuma' in env_name or 'Pitfall' in env_name:
+        env = VisitedRoomInfo(env, room_address=3 if 'Montezuma' in env_name else 1)
+
+    return env
+
 def create_atari_environment(
     env_name: str,
     seed: int = 1,
diff --git a/deep_rl_zoo/gym_env_gymnasium.py b/deep_rl_zoo/gym_env_gymnasium.py
new file mode 100644
index 0000000..13fa250
--- /dev/null
+++ b/deep_rl_zoo/gym_env_gymnasium.py
@@ -0,0 +1,1554 @@
+# The MIT License
+
+# Copyright (c) 2017 OpenAI (http://openai.com)
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+# Code adapted from openAI baselines:
+# https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py
+# Made changes to adapting coding styles, also add new function to support recording video.
+#
+# ==============================================================================
+"""gym environment processing components."""
+
+from copy import deepcopy
+from functools import partial
+import os
+import datetime
+import random
+from typing import Callable, Optional
+import numpy as np
+import cv2
+import logging
+import json
+import os.path
+import tempfile
+from typing import List, Optional
+# import gym
+# from gym.spaces import Box
+import gymnasium
+import gymnasium as gym
+from gymnasium.spaces import Box
+from collections import deque
+from pathlib import Path
+import retro
+ 
+# pylint: disable=import-error
+from deep_rl_zoo.multi_process_env import MultiProcessEnv
+import deep_rl_zoo.types as types_lib
+from gymnasium.envs.registration import EnvSpec
+
+ 
+
+# A simple list of classic env names.
+CLASSIC_ENV_NAMES = ['CartPole-v1', 'LunarLander-v2', 'MountainCar-v0', 'Acrobot-v1']
+
+
+def unwrap(env):
+    if hasattr(env, 'unwrapped'):
+        return env.unwrapped
+    elif hasattr(env, 'env'):
+        return unwrap(env.env)
+    elif hasattr(env, 'leg_env'):
+        return unwrap(env.leg_env)
+    else:
+        return env
+
+
+from gymnasium import error, logger
+
+
+class VideoRecorder:
+    """VideoRecorder renders a nice movie of a rollout, frame by frame.
+
+    It comes with an ``enabled`` option, so you can still use the same code on episodes where you don't want to record video.
+
+    Note:
+        You are responsible for calling :meth:`close` on a created VideoRecorder, or else you may leak an encoder process.
+    """
+
+    def __init__(
+        self,
+        env,
+        path: Optional[str] = None,
+        metadata: Optional[dict] = None,
+        enabled: bool = True,
+        base_path: Optional[str] = None,
+        disable_logger: bool = False,
+    ):
+        """Video recorder renders a nice movie of a rollout, frame by frame.
+
+        Args:
+            env (Env): Environment to take video of.
+            path (Optional[str]): Path to the video file; will be randomly chosen if omitted.
+            metadata (Optional[dict]): Contents to save to the metadata file.
+            enabled (bool): Whether to actually record video, or just no-op (for convenience)
+            base_path (Optional[str]): Alternatively, path to the video file without extension, which will be added.
+            disable_logger (bool): Whether to disable moviepy logger or not.
+
+        Raises:
+            Error: You can pass at most one of `path` or `base_path`
+            Error: Invalid path given that must have a particular file extension
+        """
+        # self._async = env.metadata.get("semantics.async")
+        self.enabled = enabled
+        self.disable_logger = disable_logger
+        self._closed = False
+
+        self.render_history = []
+        self.env = env
+
+        self.render_mode = env.render_mode
+
+        try:
+            # check that moviepy is now installed
+            import moviepy  # noqa: F401
+        except ImportError as e:
+            raise error.DependencyNotInstalled(
+                "moviepy is not installed, run `pip install moviepy`"
+            ) from e
+
+        if self.render_mode in {None, "human", "ansi", "ansi_list"}:
+            raise ValueError(
+                f"Render mode is {self.render_mode}, which is incompatible with"
+                f" RecordVideo. Initialize your environment with a render_mode"
+                f" that returns an image, such as rgb_array."
+            )
+
+        # Don't bother setting anything else if not enabled
+        if not self.enabled:
+            return
+
+        if path is not None and base_path is not None:
+            raise error.Error("You can pass at most one of `path` or `base_path`.")
+
+        required_ext = ".mp4"
+        if path is None:
+            if base_path is not None:
+                # Base path given, append ext
+                path = base_path + required_ext
+            else:
+                # Otherwise, just generate a unique filename
+                with tempfile.NamedTemporaryFile(suffix=required_ext) as f:
+                    path = f.name
+        self.path = path
+
+        path_base, actual_ext = os.path.splitext(self.path)
+
+        if actual_ext != required_ext:
+            raise error.Error(
+                f"Invalid path given: {self.path} -- must have file extension {required_ext}."
+            )
+
+        self.frames_per_sec = env.metadata.get("render_fps", 30)
+
+        self.broken = False
+
+        # Dump metadata
+        self.metadata = metadata or {}
+        self.metadata["content_type"] = "video/mp4"
+        self.metadata_path = f"{path_base}.meta.json"
+        self.write_metadata()
+
+        logger.info(f"Starting new video recorder writing to {self.path}")
+        self.recorded_frames = []
+
+    @property
+    def functional(self):
+        """Returns if the video recorder is functional, is enabled and not broken."""
+        return self.enabled and not self.broken
+
+    def capture_frame(self):
+        """Render the given `env` and add the resulting frame to the video."""
+        frame = self.env.render()
+        if isinstance(frame, List):
+            self.render_history += frame
+            frame = frame[-1]
+
+        if not self.functional:
+            return
+        if self._closed:
+            logger.warn(
+                "The video recorder has been closed and no frames will be captured anymore."
+            )
+            return
+        logger.debug("Capturing video frame: path=%s", self.path)
+
+        if frame is None:
+            return
+            # if self._async:
+            #     return
+            # else:
+            #     # Indicates a bug in the environment: don't want to raise
+            #     # an error here.
+            #     logger.warn(
+            #         "Env returned None on `render()`. Disabling further rendering for video recorder by marking as "
+            #         f"disabled: path={self.path} metadata_path={self.metadata_path}"
+            #     )
+            #     self.broken = True
+        else:
+            self.recorded_frames.append(frame)
+
+    def close(self):
+        """Flush all data to disk and close any open frame encoders."""
+        if not self.enabled or self._closed:
+            return
+
+        # Close the encoder
+        if len(self.recorded_frames) > 0:
+            try:
+                from moviepy.video.io.ImageSequenceClip import ImageSequenceClip
+            except ImportError as e:
+                raise error.DependencyNotInstalled(
+                    "moviepy is not installed, run `pip install moviepy`"
+                ) from e
+
+            clip = ImageSequenceClip(self.recorded_frames, fps=self.frames_per_sec)
+            moviepy_logger = None if self.disable_logger else "bar"
+            clip.write_videofile(self.path, logger=moviepy_logger)
+        else:
+            # No frames captured. Set metadata.
+            if self.metadata is None:
+                self.metadata = {}
+            self.metadata["empty"] = True
+
+        self.write_metadata()
+
+        # Stop tracking this for autoclose
+        self._closed = True
+
+    def write_metadata(self):
+        """Writes metadata to metadata path."""
+        with open(self.metadata_path, "w") as f:
+            json.dump(self.metadata, f)
+
+    def __del__(self):
+        """Closes the environment correctly when the recorder is deleted."""
+        # Make sure we've closed up shop when garbage collecting
+        if not self._closed:
+            logger.warn("Unable to save last video! Did you call close()?")
+
+
+class NoopReset(gym.Wrapper):
+    """Sample initial states by taking random number of no-ops on reset.
+    No-op is assumed to be action 0.
+    """
+
+    def __init__(self, env, noop_max=30):
+        gym.Wrapper.__init__(self, env)
+        self.noop_max = noop_max
+        self.override_num_noops = None
+        self.noop_action = 0
+        # print(env.unwrapped.get_action_meaning([0]))
+        # assert env.unwrapped.get_action_meaning([0]) == 'NOOP'
+
+    def reset(self, **kwargs):
+        """Do no-op action for a number of steps in [1, noop_max]."""
+        self.env.reset(**kwargs)
+        if self.override_num_noops is not None:
+            noops = self.override_num_noops
+        else:
+            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)  # pylint: disable=E1101
+        assert noops > 0
+        obs = None
+        for _ in range(noops):
+            obs, _, terminated, truncated, _ = self.env.step(self.noop_action)
+            done = terminated or truncated
+            if done:
+                obs, _ = self.env.reset(**kwargs)
+        return obs, {}
+
+    def step(self, action):
+        return self.env.step(action)
+
+
+class FireOnReset(gym.Wrapper):
+    """Take fire action on reset for environments like Breakout."""
+
+    def __init__(self, env):
+        gym.Wrapper.__init__(self, env)
+        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
+        assert len(env.unwrapped.get_action_meanings()) >= 3
+
+    def reset(self, **kwargs):
+        self.env.reset(**kwargs)
+        obs, _, terminated, truncated, _ = self.env.step(1)
+        done = terminated or truncated
+        if done:
+            self.env.reset(**kwargs)
+        obs, _, done, _ = self.env.step(2)
+        if done:
+            self.env.reset(**kwargs)
+        return obs
+
+    def step(self, action):
+        return self.env.step(action)
+
+
+class StickyAction(gym.Wrapper):
+    """Repeats the last action with epsilon (default 0.25) probability."""
+
+    def __init__(self, env, eps=0.25):
+        gym.Wrapper.__init__(self, env)
+        self.eps = eps
+        self.last_action = 0
+
+    def step(self, action):
+        if np.random.uniform() < self.eps:
+            action = self.last_action
+
+        self.last_action = action
+        return self.env.step(action)
+
+    def reset(self, **kwargs):
+        self.last_action = 0
+        return self.env.reset(**kwargs)
+
+
+class LifeLoss(gym.Wrapper):
+    """Adds boolean key 'loss_life' into the info dict, but only reset on true game over."""
+
+    def __init__(self, env):
+        gym.Wrapper.__init__(self, env)
+        self.lives = 0
+        self.was_real_terminated = True
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+        self.was_real_terminated = done
+        # check current lives, make loss of life terminal,
+        # then update lives to handle bonus lives
+        lives = self.env.unwrapped.ale.lives()
+
+        if lives < self.lives and lives > 0:
+            # for Qbert sometimes we stay in lives == 0 condition for a few frames
+            # so it's important to keep lives > 0, so that we only reset once
+            # the environment advertises done.
+            info['loss_life'] = True
+        else:
+            info['loss_life'] = False
+        self.lives = lives
+        return obs, reward, done, info
+
+    def reset(self, **kwargs):
+        """Reset only when lives are exhausted.
+        This way all states are still reachable even though lives are episodic,
+        and the learner need not know about any of this behind-the-scenes.
+        """
+        if self.was_real_terminated:
+            obs, _ = self.env.reset(**kwargs)
+        else:
+            # no-op step to advance from terminal/lost life state
+            obs, _, _, _ = self.env.step(0)
+        self.lives = self.env.unwrapped.ale.lives()
+        return obs
+
+
+class MaxAndSkip(gym.Wrapper):
+    """Return only every `skip`-th frame"""
+
+    def __init__(self, env, skip=4):
+        gym.Wrapper.__init__(self, env)
+        # most recent raw observations (for max pooling across time steps)
+        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)
+        self._skip = skip
+
+    def step(self, action):
+        """Repeat action, sum reward, and max over last observations."""
+        total_reward = 0.0
+        done = None
+        for i in range(self._skip):
+            obs, reward, terminated, truncated, info = self.env.step(action)
+            done = terminated or truncated
+            if i == self._skip - 2:
+                self._obs_buffer[0] = obs
+            if i == self._skip - 1:
+                self._obs_buffer[1] = obs
+            total_reward += reward
+            if done:
+                break
+        # Note that the observation on the done=True frame
+        # doesn't matter
+        max_frame = self._obs_buffer.max(axis=0)
+
+        return max_frame, total_reward, terminated, truncated, info
+
+    def reset(self, **kwargs):
+        return self.env.reset(**kwargs)
+
+
+class ResizeAndGrayscaleFrame(gym.ObservationWrapper):
+    """
+    Resize frames to 84x84, and grascale image as done in the Nature paper.
+    """
+
+    def __init__(self, env, width=84, height=84, grayscale=True):
+        super().__init__(env)
+
+        assert self.observation_space.dtype == np.uint8 and len(self.observation_space.shape) == 3
+
+        self.frame_width = width
+        self.frame_height = height
+        self.grayscale = grayscale
+        num_channels = 1 if self.grayscale else 3
+
+        self.observation_space = Box(
+            low=0,
+            high=255,
+            shape=(self.frame_height, self.frame_width, num_channels),
+            dtype=np.uint8,
+        )
+
+    def observation(self, obs):
+        # pylint: disable=no-member
+
+        if self.grayscale:
+            obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
+        obs = cv2.resize(obs, (self.frame_width, self.frame_height), interpolation=cv2.INTER_AREA)
+        # pylint: disable=no-member
+
+        if self.grayscale:
+            obs = np.expand_dims(obs, -1)
+
+        return obs
+
+
+class FrameStack(gym.Wrapper):
+    """Stack k last frames.
+    Returns lazy array, which is much more memory efficient.
+    See Also
+    --------
+    baselines.common.atari_wrappers.LazyFrames
+    """
+
+    def __init__(self, env, k):
+        gym.Wrapper.__init__(self, env)
+        self.k = k
+        self.frames = deque([], maxlen=k)
+        shape = env.observation_space.shape
+        self.observation_space = Box(low=0, high=255, shape=(shape[:-1] + (shape[-1] * k,)), dtype=env.observation_space.dtype)
+
+    def reset(self, **kwargs):
+        obs, _ = self.env.reset(**kwargs)
+        for _ in range(self.k):
+            self.frames.append(obs)
+        return self._get_obs(), {}
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+        self.frames.append(obs)
+        return self._get_obs(), reward, done, info
+
+    def _get_obs(self):
+        assert len(self.frames) == self.k
+        return LazyFrames(list(self.frames))
+
+
+class LazyFrames(object):
+    """This object ensures that common frames between the observations are only stored once.
+    It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
+    buffers.
+    This object should only be converted to numpy array before being passed to the model.
+    You'd not believe how complex the previous solution was."""
+
+    def __init__(self, frames):
+        self.dtype = frames[0].dtype
+        self.shape = (frames[0].shape[0], frames[0].shape[1], len(frames))
+        self._frames = frames
+        self._out = None
+
+    def _force(self):
+        if self._out is None:
+            self._out = np.concatenate(self._frames, axis=-1)
+            self._frames = None
+        return self._out
+
+    def __array__(self, dtype=None):
+        out = self._force()
+        if dtype is not None:
+            out = out.astype(dtype)
+        return out
+
+    def __len__(self):
+        return len(self._force())
+
+    def __getitem__(self, i):
+        return self._force()[i]
+
+    def count(self):
+        frames = self._force()
+        return frames.shape[frames.ndim - 1]
+
+    def frame(self, i):
+        return self._force()[..., i]
+
+
+class ScaleFrame(gym.ObservationWrapper):
+    """Scale frame by divide 255."""
+
+    def __init__(self, env):
+        gym.ObservationWrapper.__init__(self, env)
+        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=env.observation_space.shape, dtype=np.float32)
+
+    def observation(self, obs):
+        # careful! This undoes the memory optimization, use
+        # with smaller replay buffers only.
+        return np.array(obs).astype(np.float32) / 255.0
+
+
+class VisitedRoomInfo(gym.Wrapper):
+    """Add number of unique visited rooms to the info dictionary.
+    For Atari games like MontezumaRevenge and Pitfall.
+    """
+
+    def __init__(self, env, room_address):
+        gym.Wrapper.__init__(self, env)
+        self.room_address = room_address
+        self.visited_rooms = set()
+
+    def get_current_room(self):
+        ram = unwrap(self.env).ale.getRAM()
+        assert len(ram) == 128
+        return int(ram[self.room_address])
+
+    def step(self, action):
+        obs, rew, done, info = self.env.step(action)
+        self.visited_rooms.add(self.get_current_room())
+        if done:
+            info['episode_visited_rooms'] = len(self.visited_rooms)
+            self.visited_rooms.clear()
+        return obs, rew, done, info
+
+
+class ObscureObservation(gym.ObservationWrapper):
+    """Make the environment POMDP by obscure the state with probability epsilon.
+    this should be used before frame stack."""
+
+    def __init__(self, env, epsilon: float = 0.0):
+        super().__init__(env)
+        if not 0.0 <= epsilon < 1.0:
+            raise ValueError(f'Expect obscure epsilon should be between [0.0, 1), got {epsilon}')
+        self._eps = epsilon
+
+    def observation(self, obs):
+        if self.env.unwrapped.np_random.random() <= self._eps:
+            obs = np.zeros_like(obs, dtype=self.observation_space.dtype)
+        return obs
+
+
+class ClipRewardWithBound(gym.RewardWrapper):
+    'Clip reward to in the range [-bound, bound]'
+
+    def __init__(self, env, bound):
+        super().__init__(env)
+        self.bound = bound
+
+    def reward(self, reward):
+        return None if reward is None else max(min(reward, self.bound), -self.bound)
+
+
+class ObservationChannelFirst(gym.ObservationWrapper):
+    """Make observation image channel first, this is for PyTorch only."""
+
+    def __init__(self, env, scale_obs):
+        super().__init__(env)
+        old_shape = env.observation_space.shape
+        new_shape = (old_shape[-1], old_shape[0], old_shape[1])
+        _low, _high = (0.0, 255) if not scale_obs else (0.0, 1.0)
+        new_dtype = env.observation_space.dtype if not scale_obs else np.float32
+        self.observation_space = Box(low=_low, high=_high, shape=new_shape, dtype=new_dtype)
+
+    def observation(self, obs):
+        # permute [H, W, C] array to in the range [C, H, W]
+        # return np.transpose(observation, axes=(2, 0, 1)).astype(self.observation_space.dtype)
+        obs = np.asarray(obs, dtype=self.observation_space.dtype).transpose(2, 0, 1)
+        # make sure it's C-contiguous for compress state
+        return np.ascontiguousarray(obs, dtype=self.observation_space.dtype)
+
+
+class ObservationToNumpy(gym.ObservationWrapper):
+    """Make the observation into numpy ndarrays."""
+
+    def observation(self, obs):
+        return np.asarray(obs, dtype=self.observation_space.dtype)
+
+
+class ClipObservationWithBound(gym.ObservationWrapper):
+    """Make the observation into [-max_abs_value, max_abs_value]."""
+
+    def __init__(self, env, max_abs_value):
+        super().__init__(env)
+        self._max_abs_value = max_abs_value
+
+    def observation(self, obs):
+        return np.clip(obs, -self._max_abs_value, self._max_abs_value)
+
+
+class RecordRawReward(gym.Wrapper):
+    """This wrapper will add non-clipped/unscaled raw reward to the info dict."""
+
+    def step(self, action):
+        """Take action and add non-clipped/unscaled raw reward to the info dict."""
+
+        obs, reward, terminated, truncated, info = self.env.step(action)
+        info['raw_reward'] = reward
+
+        return obs, reward, terminated, truncated, info
+
+ 
+
+class OneHotEncoding(gym.Space):
+    """
+    
+    Based on: https://stackoverflow.com/questions/54022606/openai-gym-how-to-create-one-hot-observation-space
+    {0,...,1,...,0}
+
+    Example usage:
+    self.observation_space = OneHotEncoding(size=4)
+    """
+    def __init__(self, size=None):
+        assert isinstance(size, int) and size > 0
+        self.size = size
+        self.n=size
+        gym.Space.__init__(self, (self.size,), np.int64)
+
+    def sample(self):
+        one_hot_vector = np.array([0] * self.size) #np.zeros(self.size)
+        one_hot_vector[np.random.randint(self.size)] = 1
+        return one_hot_vector
+
+    def contains(self, x):
+        if isinstance(x, (list, tuple, np.ndarray)):
+            number_of_zeros = list(x).count(0)
+            number_of_ones = list(x).count(1)
+            return (number_of_zeros == (self.size - 1)) and (number_of_ones == 1)
+        else:
+            return False
+
+    def __repr__(self):
+        return "OneHotEncoding(%d)" % self.size
+
+    def __eq__(self, other):
+        return self.size == other.size
+
+
+class Discretizer(gym.ActionWrapper):
+    """
+    Wrap a gym environment and make it use one hot discrete actions.
+    based on https://gist.github.com/christopherhesse/8e5c63c3b0007f4c3e333c6a158872cf
+    Args:
+        combos: ordered list of lists of valid button combinations
+    """
+    REMAPPER = {
+        "atari2600": {
+            "A": "BUTTON",
+            "B": "BUTTON",
+            "ACTION_JUMP": "BUTTON",
+            "ACTION_PRIMARY": "BUTTON",
+            "ACTION_SECONDARY": "BUTTON",
+        },
+        "nes": {
+            "ACTION_JUMP": "A",
+            "ACTION_PRIMARY": "A",
+            "ACTION_SECONDARY": "B"
+        },
+        "snes": {
+            "ACTION_JUMP": "B",
+            "ACTION_PRIMARY": "B",
+            "ACTION_SECONDARY": "Y"
+        },
+        "genesis": {
+            "ACTION_JUMP": "C",
+            "ACTION_PRIMARY": "C",
+            "ACTION_SECONDARY": "B"
+        },
+        "sms": {
+            "ACTION_JUMP": "A",
+            "ACTION_PRIMARY": "A",
+            "ACTION_SECONDARY": "B"
+        },
+        "gameboy": {
+            "ACTION_JUMP": "A",
+            "ACTION_PRIMARY": "A",
+            "ACTION_SECONDARY": "B"
+        },
+        "32x": {
+            "ACTION_JUMP": "B",
+            "ACTION_PRIMARY": "B",
+            "ACTION_SECONDARY": "Y"
+        },
+    }
+
+    def __init__(self, env, platform, combos=None):
+        super().__init__(env)
+        assert isinstance(env.action_space, gym.spaces.MultiBinary)
+        buttons = env.unwrapped.buttons
+        # if len(buttons) == 0: 
+        #     buttons = ['B', 'Y', 'SELECT', 'START', 'UP', 'DOWN', 'LEFT', 'RIGHT', 'A', 'X', 'L', 'R']
+        self._decode_discrete_action = []
+        self.combos = combos
+
+        if self.combos is None:
+            self.combos = buttons 
+
+        self.combos = [combo if isinstance(combo, list) else [combo] for combo in self.combos]
+
+        try:
+            for combo in self.combos:
+                arr = np.array([0] * env.action_space.n)
+                for button in combo:
+                    button = self.REMAPPER[platform.lower()].get(button, button)
+                    if button.upper() != "NOOP":
+                        arr[buttons.index(button)] = 1
+                    # arr[button] = 1
+                    
+                self._decode_discrete_action.append(arr)
+        except:
+            raise ValueError(f"Invalid action combination {buttons}")
+        self.action_space = OneHotEncoding(len(self._decode_discrete_action))
+
+    def action(self, act):
+        
+        act = np.argmax(act)
+        return self._decode_discrete_action[act].copy()
+
+
+class MultiEnvWrapper:
+    def __init__(self, fn_env_create, games, noop_max, sticky_action, frame_skip, obscure_epsilon, terminal_on_life_loss, scale_obs, clip_reward, seed, reward_type):
+        random.seed()
+        np.random.seed()
+
+        self.noop_max = noop_max
+        self.sticky_action = sticky_action
+        self.frame_skip = frame_skip
+        self.obscure_epsilon = obscure_epsilon
+        self.terminal_on_life_loss = terminal_on_life_loss
+        self.scale_obs = scale_obs
+        self.clip_reward = clip_reward
+        self.reward_type = reward_type
+
+
+        game = random.sample(games, 1)[0]
+        print(game)
+        self.env = fn_env_create(game=game)
+        self.env = self.set_wrappers(self.env)
+        self.games = games
+        self.fn_env_create = fn_env_create
+
+    def set_wrappers(self, env):
+            
+        if self.noop_max > 0:
+            env = NoopReset(env, noop_max=self.noop_max)
+        if self.sticky_action:
+            env = StickyAction(env)
+        if self.frame_skip > 0:
+            env = MaxAndSkip(env, skip=self.frame_skip)
+
+        # Obscure observation with obscure_epsilon probability
+        if self.obscure_epsilon > 0.0:
+            env = ObscureObservation(env, self.obscure_epsilon)
+        if self.terminal_on_life_loss:
+            env = LifeLoss(env)
+
+        # env = ResizeAndGrayscaleFrame(env, width=frame_width, height=frame_height)
+
+        if self.scale_obs:
+            env = ScaleFrame(env)
+
+        if self.clip_reward:
+            env = RecordRawReward(env)
+            env = ClipRewardWithBound(env, 1.0)
+
+        return env
+
+    def __getattr__(self, name):
+        return getattr(self.env, name)
+
+    def reset(self, **kwargs):
+        self.env.close()
+        game = random.sample(self.games, 1)[0]
+        print(game)
+        self.env = self.fn_env_create(game=game)
+        self.env = self.set_wrappers(self.env)
+        return self.env.reset(**kwargs)
+
+    def step(self, *args, **kwargs):
+        return self.env.step(*args, **kwargs)
+
+
+def capped_cubic_video_schedule(episode_id: int) -> bool:
+    """The default episode trigger.
+
+    This function will trigger recordings at the episode indices 0, 1, 8, 27, ..., :math:`k^3`, ..., 729, 1000, 2000, 3000, ...
+
+    Args:
+        episode_id: The episode number
+
+    Returns:
+        If to apply a video schedule number
+    """
+    if episode_id < 1000:
+        return int(round(episode_id ** (1.0 / 3))) ** 3 == episode_id
+    else:
+        return episode_id % 1000 == 0
+    
+class RecordVideo(gym.utils.RecordConstructorArgs):
+    """This wrapper records videos of rollouts.
+
+    Usually, you only want to record episodes intermittently, say every hundredth episode.
+    To do this, you can specify **either** ``episode_trigger`` **or** ``step_trigger`` (not both).
+    They should be functions returning a boolean that indicates whether a recording should be started at the
+    current episode or step, respectively.
+    If neither :attr:`episode_trigger` nor ``step_trigger`` is passed, a default ``episode_trigger`` will be employed.
+    By default, the recording will be stopped once a `terminated` or `truncated` signal has been emitted by the environment. However, you can
+    also create recordings of fixed length (possibly spanning several episodes) by passing a strictly positive value for
+    ``video_length``.
+    """
+
+    def __init__(
+        self,
+        env: gym.Env,
+        video_folder: str,
+        episode_trigger: Callable[[int], bool] = None,
+        step_trigger: Callable[[int], bool] = None,
+        video_length: int = 0,
+        name_prefix: str = "rl-video",
+        disable_logger: bool = False,
+    ):
+        """Wrapper records videos of rollouts.
+
+        Args:
+            env: The environment that will be wrapped
+            video_folder (str): The folder where the recordings will be stored
+            episode_trigger: Function that accepts an integer and returns ``True`` iff a recording should be started at this episode
+            step_trigger: Function that accepts an integer and returns ``True`` iff a recording should be started at this step
+            video_length (int): The length of recorded episodes. If 0, entire episodes are recorded.
+                Otherwise, snippets of the specified length are captured
+            name_prefix (str): Will be prepended to the filename of the recordings
+            disable_logger (bool): Whether to disable moviepy logger or not.
+        """
+        gym.utils.RecordConstructorArgs.__init__(
+            self,
+            video_folder=video_folder,
+            episode_trigger=episode_trigger,
+            step_trigger=step_trigger,
+            video_length=video_length,
+            name_prefix=name_prefix,
+            disable_logger=disable_logger,
+        )
+        # gym.Wrapper.__init__(self, env)
+        self.env = env
+
+        # if env.render_mode in {None, "human", "ansi", "ansi_list"}:
+        #     raise ValueError(
+        #         f"Render mode is {env.render_mode}, which is incompatible with"
+        #         f" RecordVideo. Initialize your environment with a render_mode"
+        #         f" that returns an image, such as rgb_array."
+        #     )
+
+        if episode_trigger is None and step_trigger is None:
+            episode_trigger = capped_cubic_video_schedule
+
+        trigger_count = sum(x is not None for x in [episode_trigger, step_trigger])
+        assert trigger_count == 1, "Must specify exactly one trigger"
+
+        self.episode_trigger = episode_trigger
+        self.step_trigger = step_trigger
+        self.video_recorder: Optional[VideoRecorder] = None
+        self.disable_logger = disable_logger
+
+        self.video_folder = os.path.abspath(video_folder)
+        # Create output folder if needed
+        # if os.path.isdir(self.video_folder):
+        #     logger.warn(
+        #         f"Overwriting existing videos at {self.video_folder} folder "
+        #         f"(try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)"
+        #     )
+        os.makedirs(self.video_folder, exist_ok=True)
+
+        self.name_prefix = name_prefix
+        self.step_id = 0
+        self.video_length = video_length
+
+        self.recording = False
+        self.terminated = False
+        self.truncated = False
+        self.recorded_frames = 0
+        self.episode_id = 0
+
+        try:
+            self.is_vector_env = self.get_wrapper_attr("is_vector_env")
+        except AttributeError:
+            self.is_vector_env = False
+
+    def reset(self, **kwargs):
+        """Reset the environment using kwargs and then starts recording if video enabled."""
+        observations = self.env.reset(**kwargs)
+        self.terminated = False
+        self.truncated = False
+        if self.recording:
+            assert self.video_recorder is not None
+            self.video_recorder.recorded_frames = []
+            self.video_recorder.capture_frame()
+            self.recorded_frames += 1
+            if self.video_length > 0:
+                if self.recorded_frames > self.video_length:
+                    self.close_video_recorder()
+        elif self._video_enabled():
+            self.start_video_recorder()
+        return observations
+
+    def start_video_recorder(self):
+        """Starts video recorder using :class:`video_recorder.VideoRecorder`."""
+        self.close_video_recorder()
+
+        video_name = f"{self.name_prefix}-step-{self.step_id}"
+        if self.episode_trigger:
+            video_name = f"{self.name_prefix}-episode-{self.episode_id}"
+
+        base_path = os.path.join(self.video_folder, video_name)
+        self.video_recorder = VideoRecorder(
+            env=self.env,
+            base_path=base_path,
+            metadata={"step_id": self.step_id, "episode_id": self.episode_id},
+            disable_logger=self.disable_logger,
+        )
+
+        self.video_recorder.capture_frame()
+        self.recorded_frames = 1
+        self.recording = True
+
+    def _video_enabled(self):
+        if self.step_trigger:
+            return self.step_trigger(self.step_id)
+        else:
+            return self.episode_trigger(self.episode_id)
+
+    def step(self, action):
+        """Steps through the environment using action, recording observations if :attr:`self.recording`."""
+        (
+            observations,
+            rewards,
+            terminateds,
+            truncateds,
+            infos,
+        ) = self.env.step(action)
+
+        if not (self.terminated or self.truncated):
+            # increment steps and episodes
+            self.step_id += 1
+            if not self.is_vector_env:
+                if terminateds or truncateds:
+                    self.episode_id += 1
+                    self.terminated = terminateds
+                    self.truncated = truncateds
+            elif terminateds[0] or truncateds[0]:
+                self.episode_id += 1
+                self.terminated = terminateds[0]
+                self.truncated = truncateds[0]
+
+            if self.recording:
+                assert self.video_recorder is not None
+                self.video_recorder.capture_frame()
+                self.recorded_frames += 1
+                if self.video_length > 0:
+                    if self.recorded_frames > self.video_length:
+                        self.close_video_recorder()
+                else:
+                    if not self.is_vector_env:
+                        if terminateds or truncateds:
+                            self.close_video_recorder()
+                    elif terminateds[0] or truncateds[0]:
+                        self.close_video_recorder()
+
+            elif self._video_enabled():
+                self.start_video_recorder()
+
+        return observations, rewards, terminateds, truncateds, infos
+
+    def close_video_recorder(self):
+        """Closes the video recorder if currently recording."""
+        if self.recording:
+            assert self.video_recorder is not None
+            self.video_recorder.close()
+        self.recording = False
+        self.recorded_frames = 1
+
+    def render(self, *args, **kwargs):
+        """Compute the render frames as specified by render_mode attribute during initialization of the environment or as specified in kwargs."""
+        if self.video_recorder is None or not self.video_recorder.enabled:
+            return self.env.render(*args, **kwargs)
+
+        if len(self.video_recorder.render_history) > 0:
+            recorded_frames = [
+                self.video_recorder.render_history.pop()
+                for _ in range(len(self.video_recorder.render_history))
+            ]
+            if self.recording:
+                return recorded_frames
+            else:
+                return recorded_frames + self.env.render(*args, **kwargs)
+        else:
+            return self.env.render(*args, **kwargs)
+
+    def close(self):
+        """Closes the wrapper then the video recorder."""
+        self.env.close()
+        self.close_video_recorder()
+
+class TimeLimit(gym.Wrapper, gym.utils.RecordConstructorArgs):
+    """This wrapper will issue a `truncated` signal if a maximum number of timesteps is exceeded.
+
+    If a truncation is not defined inside the environment itself, this is the only place that the truncation signal is issued.
+    Critically, this is different from the `terminated` signal that originates from the underlying environment as part of the MDP.
+
+    Example:
+       >>> import gymnasium as gym
+       >>> from gymnasium.wrappers import TimeLimit
+       >>> env = gym.make("CartPole-v1")
+       >>> env = TimeLimit(env, max_episode_steps=1000)
+    """
+
+    def __init__(
+        self,
+        env: gym.Env,
+        max_episode_steps: int,
+    ):
+        """Initializes the :class:`TimeLimit` wrapper with an environment and the number of steps after which truncation will occur.
+
+        Args:
+            env: The environment to apply the wrapper
+            max_episode_steps: An optional max episode steps (if ``None``, ``env.spec.max_episode_steps`` is used)
+        """
+        gym.utils.RecordConstructorArgs.__init__(
+            self, max_episode_steps=max_episode_steps
+        )
+        gym.Wrapper.__init__(self, env)
+
+        self._max_episode_steps = max_episode_steps
+        self._elapsed_steps = None
+
+    def step(self, action):
+        """Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.
+
+        Args:
+            action: The environment step action
+
+        Returns:
+            The environment step ``(observation, reward, terminated, truncated, info)`` with `truncated=True`
+            if the number of steps elapsed >= max episode steps
+
+        """
+        observation, reward, terminated, truncated, info = self.env.step(action)
+        self._elapsed_steps += 1
+
+        if self._elapsed_steps >= self._max_episode_steps:
+            truncated = True
+
+        return observation, reward, terminated, truncated, info
+
+    def reset(self, **kwargs):
+        """Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.
+
+        Args:
+            **kwargs: The kwargs to reset the environment with
+
+        Returns:
+            The reset environment
+        """
+        self._elapsed_steps = 0
+        return self.env.reset(**kwargs)
+
+    @property
+    def spec(self) -> EnvSpec | None:
+        """Modifies the environment spec to include the `max_episode_steps=self._max_episode_steps`."""
+        if self._cached_spec is not None:
+            return self._cached_spec
+
+        env_spec = self.env.spec
+        if env_spec is not None:
+            env_spec = deepcopy(env_spec)
+            env_spec.max_episode_steps = self._max_episode_steps
+
+        self._cached_spec = env_spec
+        return env_spec
+
+def make_retro(*, game, state=None, max_episode_steps=None, skip_frames=1, render_mode="human", use_discretization=True, valid_action_combos=None, **kwargs):
+    if state is None:
+        state = retro.State.DEFAULT
+    if skip_frames < 1:
+        raise ValueError(f"skip_frames must be at least 1, got {skip_frames}")
+
+    env = retro.make(game, state, render_mode=render_mode, **kwargs)
+    # env = StochasticFrameSkip(env, n=skip_frames, stickprob=0, want_render=True)
+    if max_episode_steps is not None:
+        env = TimeLimit(env, max_episode_steps=max_episode_steps)
+    
+    if use_discretization:
+        platform = game.split("-")[1]
+        env = Discretizer(env, platform, combos=valid_action_combos)
+    return env
+
+
+def make_retro_multi(
+    games,
+    max_episode_steps=None,
+    frame_skip=1,
+    valid_action_combos=None,
+    noop_max=30,
+    sticky_action=True,
+    obscure_epsilon=0.0,
+    terminal_on_life_loss=False,
+    scale_obs=False,
+    frame_width=84,
+    frame_height=84,
+    clip_reward=True,
+    frame_stack=4,
+    channel_first=True,
+    seed=1,
+    reward_type="combined",
+):
+    
+    env_fn = partial(
+        make_retro,
+        render_mode="rgb_array",
+        valid_action_combos=valid_action_combos,
+        skip_frames=1,
+        max_episode_steps=max_episode_steps,
+    )
+    env = MultiEnvWrapper(env_fn, games, noop_max, sticky_action, frame_skip, obscure_epsilon, terminal_on_life_loss, scale_obs, clip_reward, seed, reward_type)
+
+    # Change TimeLimit wrapper to 108,000 steps (30 min) as default in the
+    # literature instead of OpenAI Gym's default of 100,000 steps.
+    #env = TimeLimit(env.env, max_episode_steps=None if max_episode_steps <= 0 else max_episode_steps)
+
+    # if noop_max > 0:
+    #     env = NoopReset(env, noop_max=noop_max)
+    # if sticky_action:
+    #     env = StickyAction(env)
+    # if frame_skip > 0:
+    #     env = MaxAndSkip(env, skip=frame_skip)
+
+    # # Obscure observation with obscure_epsilon probability
+    # if obscure_epsilon > 0.0:
+    #     env = ObscureObservation(env, obscure_epsilon)
+    # if terminal_on_life_loss:
+    #     env = LifeLoss(env)
+
+    # # env = ResizeAndGrayscaleFrame(env, width=frame_width, height=frame_height)
+
+    # if scale_obs:
+    #     env = ScaleFrame(env)
+
+    # if clip_reward:
+    #     env = RecordRawReward(env)
+    #     env = ClipRewardWithBound(env, 1.0)
+
+    # print(f"Frame stack: {frame_stack}")
+    # if frame_stack > 1:
+    #     env = FrameStack(env, frame_stack)
+    # if channel_first:
+    #     env = ObservationChannelFirst(env, scale_obs)
+    # else:
+    #     # This is required as LazeFrame object is not numpy.array.
+    #     env = ObservationToNumpy(env)
+
+    return env
+
+def create_retro(game, seed, num_envs, transform=None, extras=dict()):
+    # env_fn = partial(instantiate, config=cfg_env)
+    games = game.split(',')#["SuperMarioBros-Nes"] #["AdventureIslandII-Nes"] #get_game_list()
+    valid_action_combos = ["RIGHT", "LEFT", "UP", "DOWN", "ACTION_JUMP"]
+
+    max_episode_steps=extras["max_episode_steps"]
+    frame_skip=extras["frame_skip"]
+    noop_max=extras["noop_max"]
+    sticky_action=extras["sticky_action"]
+    obscure_epsilon=extras["obscure_epsilon"]
+    terminal_on_life_loss=extras["terminal_on_life_loss"]
+    scale_obs=extras["scale_obs"]
+    frame_width=extras["frame_width"]
+    frame_height=extras["frame_height"]
+    clip_reward=extras["clip_reward"]
+    frame_stack=extras["frame_stack"]
+    channel_first=extras["channel_first"]
+    reward_type=extras["reward_type"]
+    
+    fn_make_env = partial(
+        make_retro_multi,
+        games=games,
+        max_episode_steps=max_episode_steps,
+        frame_skip=frame_skip,
+        valid_action_combos=valid_action_combos,
+        noop_max=noop_max,
+        sticky_action=sticky_action,
+        obscure_epsilon=obscure_epsilon,
+        terminal_on_life_loss=terminal_on_life_loss,
+        scale_obs=scale_obs,
+        frame_width=frame_width,
+        frame_height=frame_height,
+        clip_reward=clip_reward,
+        frame_stack=frame_stack,
+        channel_first=channel_first,
+        seed=seed,
+        reward_type=reward_type
+    )
+
+    env = MultiProcessEnv(fn_make_env, num_envs, should_wait_num_envs_ratio=0.5, transform=transform, id=game)
+    return env
+
+
+def create_retro_environment(
+    env_name: str,
+    seed: int = 1,
+    frame_skip: int = 4,
+    frame_stack: int = 4,
+    frame_height: int = 84,
+    frame_width: int = 84,
+    noop_max: int = 30,
+    max_episode_steps: int = 108000,
+    obscure_epsilon: float = 0.0,
+    terminal_on_life_loss: bool = False,
+    clip_reward: bool = True,
+    sticky_action: bool = True,
+    scale_obs: bool = False,
+    channel_first: bool = True,
+    reward_type: str = "combined",
+) -> gymnasium.Env:
+    """
+    Process gym env for Atari games according to the Nature DQN paper.
+
+    Args:
+        env_name: the environment name without 'NoFrameskip' and version.
+        seed: seed the runtime.
+        frame_skip: the frequency at which the agent experiences the game,
+                the environment will also repeat action.
+        frame_stack: stack n last frames.
+        frame_height: height of the resized frame.
+        frame_width: width of the resized frame.
+        noop_max: maximum number of no-ops to apply at the beginning
+                of each episode to reduce determinism. These no-ops are applied at a
+                low-level, before frame skipping.
+        max_episode_steps: maximum steps for an episode.
+        obscure_epsilon: with epsilon probability [0.0, 1.0), obscure the state to make it POMDP.
+        terminal_on_life_loss: if True, mark end of game when loss a life, default off.
+        clip_reward: clip reward in the range of [-1, 1], default on.
+        sticky_action: if True, randomly re-use last action with 0.25 probability, default on.
+        scale_obs: scale the frame by divide 255, turn this on may require 4-5x more RAM when using experience replay, default off.
+        channel_first: if True, change observation image from shape [H, W, C] to in the range [C, H, W], this is for PyTorch only, default on.
+
+    Returns:
+        preprocessed gym.Env for Atari games.
+    """
+    if 'NoFrameskip' in env_name:
+        raise ValueError(f'Environment name should not include NoFrameskip, got {env_name}')
+
+    extras = {
+        "noop_max": noop_max,
+        "sticky_action": sticky_action,
+        "frame_skip": frame_skip,
+        "obscure_epsilon": obscure_epsilon,
+        "terminal_on_life_loss": terminal_on_life_loss,
+        "scale_obs": scale_obs,
+        "frame_width": frame_width,
+        "frame_height": frame_height,
+        "clip_reward": clip_reward,
+        "frame_stack": frame_stack,
+        "channel_first": channel_first,
+        "max_episode_steps": max_episode_steps,
+        "reward_type": reward_type
+    }
+
+    env = create_retro(env_name, seed, 1, extras=extras) #gym.make(f'{env_name}NoFrameskip-v4')
+    # obs = env.reset()
+    # env.step(0)
+    # print("OBS", obs.shape)
+    # exit()`
+    # unwrap(env).seed(seed)
+
+
+    # # Change TimeLimit wrapper to 108,000 steps (30 min) as default in the
+    # # literature instead of OpenAI Gym's default of 100,000 steps.
+    # env = gymnasium.wrappers.TimeLimit(env.env, max_episode_steps=None if max_episode_steps <= 0 else max_episode_steps)
+
+    # if noop_max > 0:
+    #     env = NoopReset(env, noop_max=noop_max)
+    # if sticky_action:
+    #     env = StickyAction(env)
+    # if frame_skip > 0:
+    #     env = MaxAndSkip(env, skip=frame_skip)
+
+    # # Obscure observation with obscure_epsilon probability
+    # if obscure_epsilon > 0.0:
+    #     env = ObscureObservation(env, obscure_epsilon)
+    # if terminal_on_life_loss:
+    #     env = LifeLoss(env)
+
+    # env = ResizeAndGrayscaleFrame(env, width=frame_width, height=frame_height)
+
+    # if scale_obs:
+    #     env = ScaleFrame(env)
+
+    # if clip_reward:
+    #     env = RecordRawReward(env)
+    #     env = ClipRewardWithBound(env, 1.0)
+
+    # if frame_stack > 1:
+    #     env = FrameStack(env, frame_stack)
+    # if channel_first:
+    #     env = ObservationChannelFirst(env, scale_obs)
+    # else:
+    #     # This is required as LazeFrame object is not numpy.array.
+    #     env = ObservationToNumpy(env)
+
+    # if 'Montezuma' in env_name or 'Pitfall' in env_name:
+    #     env = VisitedRoomInfo(env, room_address=3 if 'Montezuma' in env_name else 1)
+
+    return env
+
+
+def create_classic_environment(
+    env_name: str,
+    seed: int = 1,
+    max_abs_reward: int = None,
+    obscure_epsilon: float = 0.0,
+) -> gym.Env:
+    """
+    Process gym env for classic control tasks like CartPole, LunarLander, MountainCar
+
+    Args:
+        env_name: the environment name with version attached.
+        seed: seed the runtime.
+        max_abs_reward: clip reward in the range of [-max_abs_reward, max_abs_reward], default off.
+        obscure_epsilon: with epsilon probability [0.0, 1.0) obscure the state to make it POMDP.
+
+    Returns:
+        gym.Env for classic control tasks
+    """
+
+    env = gym.make(env_name)
+    # BUG
+    # when using distributed RL with multiprocessing, and we try to seed for classic control environments, we get the following error:
+    # File "/usr/lib/python3.10/multiprocessing/spawn.py", line 126, in _main
+    #     self = reduction.pickle.load(from_parent)
+    # TypeError: RandomNumberGenerator._generator_ctor() takes from 0 to 1 positional arguments but 2 were given
+    #
+    # Seems like this issue only affects classic control tasks like Cart Pole etc., we don't have this problem on Atari
+
+    # env.seed(seed)
+
+    # Clip reward to max absolute reward bound
+    if max_abs_reward is not None:
+        env = RecordRawReward(env)
+        env = ClipRewardWithBound(env, abs(max_abs_reward))
+
+    # Obscure observation with obscure_epsilon probability
+    if obscure_epsilon > 0.0:
+        env = ObscureObservation(env, obscure_epsilon)
+
+    return env
+
+
+def create_continuous_environment(
+    env_name: str,
+    seed: int = 1,
+    max_abs_obs: int = 10,
+    max_abs_reward: int = 10,
+) -> gym.Env:
+    """
+    Process gym env for classic robotic control tasks like Humanoid, Ant.
+
+    Args:
+        env_name: the environment name with version attached.
+        seed: seed the runtime.
+        max_abs_obs: clip observation in the range of [-max_abs_obs, max_abs_obs], default 10.
+        max_abs_reward: clip reward in the range of [-max_abs_reward, max_abs_reward], default 10.
+
+    Returns:
+        gym.Env for classic robotic control tasks
+    """
+
+    env = gym.make(env_name)
+
+    env = gym.wrappers.ClipAction(env)
+    env = gym.wrappers.NormalizeObservation(env)
+
+    env = RecordRawReward(env)
+
+    env = gym.wrappers.NormalizeReward(env)
+
+    # Optionally clipping the observation and rewards.
+    # Notice using lambda function does not work with python multiprocessing
+    # env = gym.wrappers.TransformObservation(env, lambda reward: np.clip(obs, -max_abs_obs, max_abs_obs))
+    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -max_abs_reward, max_abs_reward))
+    env = ClipObservationWithBound(env, max_abs_obs)
+    env = ClipRewardWithBound(env, max_abs_reward)
+
+    env.seed(seed)
+    env.action_space.seed(seed)
+    env.observation_space.seed(seed)
+    return env
+
+
+# def play_and_record_video(
+#     agent: types_lib.Agent,
+#     env,
+#     save_dir: str = './recordings',
+# ) -> None:
+#     """Self-play and record a video for a single game.
+
+#     Args:
+#         env: the gym environment to play.
+#         agent: the agent which should have step() method to return action for a given state.
+#         save_dir: the recording video file directory, default save to 'recordings/{agent_name}_{env.spec.id}_{timestamp}'.
+
+#     Raises:
+#         if agent is not an instance of types_lib.Agent.
+#     """
+
+#     if not isinstance(agent, types_lib.Agent):
+#         raise RuntimeError('Expect agent to have a callable step() method.')
+
+#     # Create dir if needed
+#     if save_dir is not None and save_dir != '' and not os.path.exists(save_dir):
+#         _dir = Path(save_dir)
+#         _dir.mkdir(parents=True, exist_ok=False)
+
+#     assert os.path.exists(save_dir) and os.path.isdir(save_dir)
+
+#     # Create a sub folder with name env.id + timestamp
+#     ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
+#     full_save_dir = os.path.join(save_dir, f'{agent.agent_name}_{env.spec.id}_{ts}')
+#     logging.info(f'Recording self-play video at "{full_save_dir}"')
+
+#     # env = RecordVideo(env, full_save_dir)
+
+#     observation, _ = env.reset()
+#     agent.reset()
+
+#     reward = 0.0
+#     done = False
+#     first_step = True
+
+#     t = 0
+
+#     while True:
+#         timestep_t = types_lib.TimeStep(
+#             observation=observation,
+#             reward=reward,
+#             done=done,
+#             first=first_step,
+#             info=None,  # No tracking here
+#         )
+#         a_t = agent.step(timestep_t)
+#         observation, reward, done, _ = env.step(a_t)
+#         t += 1
+
+#         first_step = False
+#         if done:
+#             break
+
+#     env.close()
+
+import os
+import datetime
+import logging
+from pathlib import Path
+import imageio
+import numpy as np
+# Add any other necessary imports here
+
+def play_and_record_video(
+    agent: types_lib.Agent,
+    env,
+    save_dir: str = './recordings',
+) -> None:
+    """Self-play and record a video for a single game.
+
+    Args:
+        env: the gym environment to play.
+        agent: the agent which should have step() method to return action for a given state.
+        save_dir: the recording video file directory, default save to 'recordings/{agent_name}_{env.spec.id}_{timestamp}'.
+
+    Raises:
+        RuntimeError: if agent is not an instance of types_lib.Agent.
+    """
+
+    if not isinstance(agent, types_lib.Agent):
+        raise RuntimeError('Expect agent to have a callable step() method.')
+
+    # Create dir if needed
+    if save_dir and not os.path.exists(save_dir):
+        _dir = Path(save_dir)
+        _dir.mkdir(parents=True, exist_ok=False)
+
+    assert os.path.exists(save_dir) and os.path.isdir(save_dir)
+
+    # Create a subfolder with name agent_name_env_id_timestamp
+    ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
+    full_save_dir = os.path.join(save_dir, f'{agent.agent_name}_{env.spec.id}_{ts}')
+    logging.info(f'Recording self-play video at {full_save_dir}')
+
+    # Create the full_save_dir if it doesn't exist
+    os.makedirs(full_save_dir, exist_ok=True)
+
+    # Initialize list to store frames
+    frames = []
+
+    observation = env.reset()
+    agent.reset()
+
+    reward = 0.0
+    done = False
+    first_step = True
+
+    t = 0
+
+    while True:
+        timestep_t = types_lib.TimeStep(
+            observation=observation,
+            reward=reward,
+            done=done,
+            first=first_step,
+            info=None,  # No tracking here
+        )
+        a_t = agent.step(timestep_t)
+        observation, reward, done, _ = env.step(a_t)
+
+        # Append the observation to frames
+        frames.append(observation)
+
+        t += 1
+        first_step = False
+
+        if done:
+            break
+
+    env.close()
+
+    # Save frames as video
+    video_filename = os.path.join(full_save_dir, 'video.mp4')
+    fps = 30  # Adjust the FPS as needed
+
+    # Ensure frames are in uint8 format
+    frames = [frame.transpose(1, 2, 0).astype(np.uint8) for frame in frames]
+
+    print(f"Writing video to {video_filename}")
+    # Write the frames to a video file
+    imageio.mimsave(video_filename, frames, fps=fps)
diff --git a/deep_rl_zoo/main_loop_gymnasium.py b/deep_rl_zoo/main_loop_gymnasium.py
new file mode 100644
index 0000000..965a3c4
--- /dev/null
+++ b/deep_rl_zoo/main_loop_gymnasium.py
@@ -0,0 +1,704 @@
+# Copyright 2022 The Deep RL Zoo Authors. All Rights Reserved.
+# Copyright 2019 DeepMind Technologies Limited. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# The functions 'run_env_loop' has been modified
+# by The Deep RL Zoo Authors to support gym environment
+# without DeepMin's dm.env wrapper.
+#
+# ==============================================================================
+"""Training loops for Deep RL Zoo."""
+from collections.abc import Callable
+from typing import Iterable, List, Tuple, Text, Mapping, Any
+import itertools
+import collections
+import sys
+import time
+import signal
+import queue
+import math
+import multiprocessing
+import threading
+from absl import logging
+import gymnasium as gym
+import os
+
+# pylint: disable=import-error
+import deep_rl_zoo.trackers as trackers_lib
+import deep_rl_zoo.types as types_lib
+from deep_rl_zoo.log import CsvWriter
+from deep_rl_zoo.checkpoint import PyTorchCheckpoint
+from deep_rl_zoo import gym_env_gymnasium as gym_env
+
+
+def run_env_loop(
+    agent: types_lib.Agent, env: gym.Env
+) -> Iterable[Tuple[gym.Env, types_lib.TimeStep, types_lib.Agent, types_lib.Action]]:
+    """Repeatedly alternates step calls on environment and agent.
+
+    At time `t`, `t + 1` environment timesteps and `t + 1` agent steps have been
+    seen in the current episode. `t` resets to `0` for the next episode.
+
+    Args:
+      agent: Agent to be run, has methods `step(timestep)` and `reset()`.
+      env: Environment to run, has methods `step(action)` and `reset()`.
+
+    Yields:
+      Tuple `(env, timestep_t, agent, a_t)` where
+        `a_t = agent.step(timestep_t)`.
+
+    Raises:
+        RuntimeError if the `agent` is not an instance of types_lib.Agent.
+    """
+
+    if not isinstance(agent, types_lib.Agent):
+        raise RuntimeError('Expect agent to be an instance of types_lib.Agent.')
+
+    while True:  # For each episode.
+        logging.info('Starting new episode.')
+        agent.reset()
+        
+        # Think of reset as a special 'action' the agent takes, thus given us a reward 'zero', and a new state 's_t'.
+        observation = env.reset()
+        
+        reward = 0.0
+        done = loss_life = False
+        first_step = True
+        info = {}
+
+        while True:  # For each step in the current episode.
+            timestep_t = types_lib.TimeStep(
+                observation=observation,
+                reward=reward,
+                done=done or loss_life,
+                first=first_step,
+                info=info,
+            )
+            a_t = agent.step(timestep_t)
+            yield env, timestep_t, agent, a_t
+
+            a_tm1 = a_t
+            observation, reward, done, info = env.step(a_tm1)
+
+            first_step = False
+
+            # For Atari games, check if should treat loss a life as a soft-terminal state
+            # loss_life = False
+            # if 'loss_life' in info and info['loss_life']:
+            #     loss_life = info['loss_life']
+
+            if done:  # Actual end of an episode
+                # This final agent.step() will ensure the done state and final reward will be seen by the agent and the trackers
+                timestep_t = types_lib.TimeStep(
+                    observation=observation,
+                    reward=reward,
+                    done=True,
+                    first=False,
+                    info=info,
+                )
+                unused_a = agent.step(timestep_t)  # noqa: F841
+                yield env, timestep_t, agent, None
+                break
+
+
+def run_env_steps(num_steps: int, agent: types_lib.Agent, env: gym.Env, trackers: Iterable[Any]) -> Mapping[Text, float]:
+    """Run some steps and return the statistics, this could be either training, evaluation, or testing steps.
+
+    Args:
+        max_episode_steps: maximum steps per episode.
+        agent: agent to run, expect the agent to have step(), reset(), and a agent_name property.
+        train_env: training environment.
+        trackers: statistics trackers.
+
+    Returns:
+        A Dict contains statistics about the result.
+
+    """
+    seq = run_env_loop(agent, env)
+    seq_truncated = itertools.islice(seq, num_steps)
+    stats = trackers_lib.generate_statistics(trackers, seq_truncated)
+    return stats
+
+
+def run_single_thread_training_iterations(
+    num_iterations: int,
+    num_train_steps: int,
+    num_eval_steps: int,
+    train_agent: types_lib.Agent,
+    train_env: gym.Env,
+    eval_agent: types_lib.Agent,
+    eval_env: gym.Env,
+    checkpoint: PyTorchCheckpoint,
+    csv_file: str,
+    use_tensorboard: bool,
+    tag: str = None,
+    debug_screenshots_interval: int = 0,
+    root_dir: str = None,
+) -> None:
+    """Runs single-thread training and evaluation for N iterations.
+    The same code structure is shared by most single-threaded DQN agents,
+    and some policy gradients agents like reinforce, actor-critic.
+
+    For every iteration:
+        1. Start to run agent for num_train_steps training environment steps/frames.
+        2. Create checkpoint file.
+        3. (Optional) Run some evaluation steps with a separate evaluation actor and environment.
+
+    Args:
+        num_iterations: number of iterations to run.
+        num_train_steps: number of frames (or env steps) to run, per iteration.
+        num_eval_steps: number of evaluation frames (or env steps) to run, per iteration.
+        train_agent: training agent, expect the agent to have step(), reset(), and a agent_name property.
+        train_env: training environment.
+        eval_agent: evaluation agent.
+        eval_env: evaluation environment.
+        checkpoint: checkpoint object.
+        csv_file: csv log file path and name.
+        use_tensorboard: if True, use tensorboard to log the runs.
+        tag: tensorboard run log tag, default None.
+        debug_screenshots_interval: the frequency to take screenshots and add to tensorboard, default 0 no screenshots.
+
+    """
+    assert root_dir is not None, "root_dir must be provided"
+    # root_dir = f"/scratch/nedko_savov/projects/ivg/external/open-genie/agent57"
+    os.makedirs(root_dir, exist_ok=True)
+    # Create log file writer.
+    writer = CsvWriter(csv_file)
+
+    # Create trackers for training and evaluation
+    train_tb_log_prefix = (
+        get_tb_log_prefix(train_env.spec.id, train_agent.agent_name, tag, 'train') if use_tensorboard else None
+    )
+    train_trackers = trackers_lib.make_default_trackers(root_dir, train_tb_log_prefix, debug_screenshots_interval)
+
+    should_run_evaluator = False
+    eval_trackers = None
+    if num_eval_steps > 0 and eval_agent is not None and eval_env is not None:
+        should_run_evaluator = True
+        eval_tb_log_prefix = (
+            get_tb_log_prefix(eval_env.spec.id, eval_agent.agent_name, tag, 'eval') if use_tensorboard else None
+        )
+        eval_trackers = trackers_lib.make_default_trackers(root_dir, eval_tb_log_prefix, debug_screenshots_interval)
+
+    # Start training
+    for iteration in range(1, num_iterations + 1):
+        logging.info(f'Training iteration {iteration}')
+
+        # Run training steps.
+        train_stats = run_env_steps(num_train_steps, train_agent, train_env, train_trackers)
+
+        checkpoint.set_iteration(iteration)
+        saved_ckpt = checkpoint.save()
+
+        if saved_ckpt:
+            logging.info(f'New checkpoint created at "{saved_ckpt}"')
+
+        # Logging training statistics.
+        log_output = [
+            ('iteration', iteration, '%3d'),
+            ('train_step', iteration * num_train_steps, '%5d'),
+            ('train_episode_return', train_stats['mean_episode_return'], '%2.2f'),
+            ('train_num_episodes', train_stats['num_episodes'], '%3d'),
+            ('train_step_rate', train_stats['step_rate'], '%4.0f'),
+            ('train_duration', train_stats['duration'], '%.2f'),
+        ]
+
+        # Run evaluation steps.
+        if should_run_evaluator is True:
+            logging.info(f'Evaluation iteration {iteration}')
+
+            # Run some evaluation steps.
+            eval_stats = run_env_steps(num_eval_steps, eval_agent, eval_env, eval_trackers)
+
+            # Logging evaluation statistics.
+            eval_output = [
+                ('eval_step', iteration * num_eval_steps, '%5d'),
+                ('eval_episode_return', eval_stats['mean_episode_return'], '% 2.2f'),
+                ('eval_num_episodes', eval_stats['num_episodes'], '%3d'),
+                ('eval_step_rate', eval_stats['step_rate'], '%4.0f'),
+                ('eval_duration', eval_stats['duration'], '%.2f'),
+            ]
+            log_output.extend(eval_output)
+
+        log_output_str = ', '.join(('%s: ' + f) % (n, v) for n, v, f in log_output)
+        logging.info(log_output_str)
+        writer.write(collections.OrderedDict((n, v) for n, v, _ in log_output))
+    writer.close()
+
+
+def run_parallel_training_iterations(
+    num_iterations: int,
+    num_train_steps: int,
+    num_eval_steps: int,
+    learner_agent: types_lib.Learner,
+    eval_agent: types_lib.Agent,
+    eval_env: gym.Env,
+    actors: List[types_lib.Agent],
+    actor_envs: List[Callable[[Any], Any]],
+    data_queue: multiprocessing.Queue,
+    checkpoint: PyTorchCheckpoint,
+    csv_file: str,
+    use_tensorboard: bool,
+    tag: str = None,
+    debug_screenshots_interval: int = 0,
+    root_dir: str = None,
+) -> None:
+    """This is the place to kick start parallel training with multiple actors processes and a single learner process.
+    The actual flow is controlled by `run_learner`.
+
+    Args:
+        num_iterations: number of iterations to run.
+        num_train_steps: number of frames (or env steps) to run, per iteration.
+        num_eval_steps: number of evaluation frames (or env steps) to run, per iteration.
+        learner_agent: learner agent, expect the agent to have run_train_loop() method.
+        eval_agent: evaluation agent.
+        eval_env: evaluation environment.
+        actors: list of actor instances we wish to run.
+        actor_envs: list of gym.Env for each actor to run.
+        data_queue: a multiprocessing.Queue used to receive transition samples from actors.
+        checkpoint: checkpoint object.
+        csv_file: csv log file path and name.
+        use_tensorboard: if True, use tensorboard to log the runs.
+        tag: tensorboard run log tag, default None.
+        debug_screenshots_interval: the frequency to take screenshots and add to tensorboard, default 0 no screenshots.
+
+    """
+
+    # Create shared iteration count and start, end training event.
+    # start_iteration_event is used to signaling actors to run one training iteration,
+    # stop_event is used to signaling actors the end of training session.
+    # The start_iteration_event and stop_event are only set by the main process.
+    iteration_count = multiprocessing.Value('i', 0)
+    start_iteration_event = multiprocessing.Event()
+    stop_event = multiprocessing.Event()
+
+    # To get training statistics from each actor and the learner. We use a single writer to write to csv file.
+    log_queue = multiprocessing.SimpleQueue()
+
+    # Run learner train loop on a new thread.
+    learner = threading.Thread(
+        target=run_learner,
+        args=(
+            num_iterations,
+            num_eval_steps,
+            learner_agent,
+            eval_agent,
+            eval_env,
+            data_queue,
+            log_queue,
+            iteration_count,
+            start_iteration_event,
+            stop_event,
+            checkpoint,
+            len(actors),
+            use_tensorboard,
+            tag,
+            root_dir,
+        ),
+    )
+    learner.start()
+
+    # Start logging on a new thread, since it's very light-weight task.
+    logger = threading.Thread(
+        target=run_logger,
+        args=(log_queue, csv_file),
+    )
+    logger.start()
+
+    # Create and start actor processes once, this will preserve actor's internal state like steps etc.
+    # Tensorboard log dir prefix.
+    num_actors = len(actors)
+    actor_tb_log_prefixes = [None for _ in range(num_actors)]
+    if use_tensorboard:
+        # To get better performance, we only log a maximum of 8 actor statistics to tensorboard
+        _step = 1 if num_actors <= 8 else math.ceil(num_actors / 8)
+        for i in range(0, num_actors, _step):
+            actor_tb_log_prefixes[i] = get_tb_log_prefix(eval_env.spec.id, actors[i].agent_name, tag, 'train')
+
+    processes = []
+    for actor, actor_env, tb_log_prefix in zip(actors, actor_envs, actor_tb_log_prefixes):
+        p = multiprocessing.Process(
+            target=run_actor,
+            args=(
+                actor,
+                actor_env,
+                data_queue,
+                log_queue,
+                num_train_steps,
+                iteration_count,
+                start_iteration_event,
+                stop_event,
+                tb_log_prefix,
+                debug_screenshots_interval,
+                root_dir,
+            ),
+        )
+        p.start()
+        processes.append(p)
+
+    # Wait for all actor to be finished.
+    for p in processes:
+        p.join()
+        p.close()
+
+    # learner.join()
+    logger.join()
+
+    # Close queue.
+    data_queue.close()
+
+
+def run_actor(
+    actor: types_lib.Agent,
+    actor_env: Callable[[Any], Any],
+    data_queue: multiprocessing.Queue,
+    log_queue: multiprocessing.SimpleQueue,
+    num_train_steps: int,
+    iteration_count: multiprocessing.Value,
+    start_iteration_event: multiprocessing.Event,
+    stop_event: multiprocessing.Event,
+    tb_log_prefix: str = None,
+    debug_screenshots_interval: int = 0,
+    root_dir: str = None,
+) -> None:
+    """
+    Run actor process for as long as required, only terminate if the `stop_event` is set to True.
+    Which is set by the main process.
+
+    * Each actor will wait for the `start_iteration_event` signal to start run num_train_steps steps (for one iteration).
+    * The actor whoever finished the current iteration first will reset `start_iteration_event` to False,
+    so it does not run into a loop that is out of control.
+
+    Args:
+        actor: the actor to run.
+        actor_env: environment for the actor instance.
+        data_queue: multiprocessing.Queue used for transferring data from actor to learner.
+        log_queue: multiprocessing.SimpleQueue used for transferring training statistics from actor,
+            this is only for write to csv file, not for tensorboard.
+        num_train_steps: number of frames (or env steps) to run for one iteration.
+        iteration: a counter which is updated by the main process.
+        start_iteration_event: start training signal, set by the main process, clear by actor.
+        stop_event: end training signal.
+        tb_log_prefix: tensorboard run log dir prefix.
+        debug_screenshots_interval: the frequency to take screenshots and add to tensorboard, default 0 no screenshots.
+
+    Raises:
+        RuntimeError if the `actor` is not a instance of types_lib.Agent.
+    """
+    if not isinstance(actor, types_lib.Agent):
+        raise RuntimeError('Expect actor to be a instance of types_lib.Agent.')
+
+    assert root_dir is not None, "root_dir must be provided"
+    # root_dir = "/scratch/nedko_savov/projects/ivg/external/open-genie/agent57"
+    actor_env = actor_env()
+    # Initialize logging.
+    init_absl_logging()
+
+    # Listen to signals to exit process.
+    handle_exit_signal()
+
+    actor_trackers = trackers_lib.make_default_trackers(root_dir, tb_log_prefix, debug_screenshots_interval)
+
+    while not stop_event.is_set():
+        # Wait for start training event signal, which is set by the main process.
+        if not start_iteration_event.is_set():
+            continue
+
+        logging.info(f'Starting {actor.agent_name} ...')
+        iteration = iteration_count.value
+
+        # Run training steps.
+        train_stats = run_env_steps(num_train_steps, actor, actor_env, actor_trackers)
+
+        # Mark work done to avoid infinite loop in `run_learner_loop`,
+        # also possible multiprocessing.Queue deadlock.
+        data_queue.put('PROCESS_DONE')
+
+        # Whoever finished one iteration first will clear the start training event.
+        if start_iteration_event.is_set():
+            start_iteration_event.clear()
+
+        # Logging statistics after training finished
+        log_output = [
+            ('iteration', iteration, '%3d'),
+            ('role', actor.agent_name, '%2s'),
+            ('step', iteration * num_train_steps, '%5d'),
+            ('episode_return', train_stats['mean_episode_return'], '% 2.2f'),
+            ('num_episodes', train_stats['num_episodes'], '%3d'),
+            ('step_rate', train_stats['step_rate'], '%4.0f'),
+            ('duration', train_stats['duration'], '%.2f'),
+        ]
+
+        # Add training statistics to log queue, so the logger process can write to csv file.
+        log_queue.put(log_output)
+
+
+def run_learner(
+    num_iterations: int,
+    num_eval_steps: int,
+    learner: types_lib.Learner,
+    eval_agent: types_lib.Agent,
+    eval_env: gym.Env,
+    data_queue: multiprocessing.Queue,
+    log_queue: multiprocessing.SimpleQueue,
+    iteration_count: multiprocessing.Value,
+    start_iteration_event: multiprocessing.Event,
+    stop_event: multiprocessing.Event,
+    checkpoint: PyTorchCheckpoint,
+    num_actors: int,
+    use_tensorboard: bool,
+    tag: str = None,
+    root_dir: str = None,
+) -> None:
+    """Run learner for N iterations.
+
+    For every iteration:
+        1. Signal actors to start a new iteration.
+        2. Start to run the learner loop until all actors are finished their work.
+        3. Create checkpoint file.
+        4. (Optional) Run evaluation steps with a separate evaluation actor and environment.
+
+    At the beginning of every iteration, learner will set the `start_iteration_event` to True, to signal actors to start training.
+    The actor whoever finished the iteration first will reset `start_iteration_event` to False.
+    Then on the next iteration, the learner will set the `start_iteration_event` to True.
+
+    Args:
+        num_iterations: number of iterations to run.
+        num_eval_steps: number of evaluation frames (or env steps) to run, per iteration.
+        learner: learner agent, expect the agent to have run_train_loop() method.
+        eval_agent: evaluation agent.
+        eval_env: evaluation environment.
+        data_queue: a multiprocessing.Queue used receive samples from actor.
+        log_queue: a multiprocessing.SimpleQueue used send evaluation statistics to logger.
+        start_iteration_event: a multiprocessing.Event signal to actors for start training.
+        checkpoint: checkpoint object.
+        num_actors: number of actors running, used to check if one iteration is over.
+        use_tensorboard: if True, use tensorboard to log the runs.
+        tag: tensorboard run log tag.
+
+    Raises:
+        RuntimeError if the `learner` is not a instance of types_lib.Learner.
+    """
+    if not isinstance(learner, types_lib.Learner):
+        raise RuntimeError('Expect learner to be a instance of types_lib.Learner.')
+
+    assert root_dir is not None, "root_dir must be provided"
+    # root_dir = "/scratch/nedko_savov/projects/ivg/external/open-genie/agent57"
+    # Create trackers for learner and evaluator
+    learner_tb_log_prefix = get_tb_log_prefix(eval_env.spec.id, learner.agent_name, tag, 'train') if use_tensorboard else None
+    learner_trackers = trackers_lib.make_learner_trackers(root_dir, learner_tb_log_prefix)
+    for tracker in learner_trackers:
+        tracker.reset()
+
+    should_run_evaluator = False
+    eval_trackers = None
+    if num_eval_steps > 0 and eval_agent is not None and eval_env is not None:
+        should_run_evaluator = True
+        eval_tb_log_prefix = (
+            get_tb_log_prefix(eval_env.spec.id, eval_agent.agent_name, tag, 'eval') if use_tensorboard else None
+        )
+        eval_trackers = trackers_lib.make_default_trackers(root_dir, eval_tb_log_prefix)
+
+    # Start training
+    for iteration in range(1, num_iterations + 1):
+        logging.info(f'Training iteration {iteration}')
+        logging.info(f'Starting {learner.agent_name} ...')
+
+        # Update shared iteration count.
+        iteration_count.value = iteration
+
+        # Set start training event.
+        start_iteration_event.set()
+        learner.reset()
+
+        run_learner_loop(learner, data_queue, num_actors, learner_trackers)
+
+        start_iteration_event.clear()
+        checkpoint.set_iteration(iteration)
+        saved_ckpt = checkpoint.save()
+
+        if saved_ckpt:
+            logging.info(f'New checkpoint created at "{saved_ckpt}"')
+
+        # Run evaluation steps.
+        if should_run_evaluator is True:
+            logging.info(f'Evaluation iteration {iteration}')
+
+            # Run some evaluation steps.
+            eval_stats = run_env_steps(num_eval_steps, eval_agent, eval_env, eval_trackers)
+
+            # Logging evaluation statistics
+            log_output = [
+                ('iteration', iteration, '%3d'),
+                ('role', 'evaluation', '%3s'),
+                ('step', iteration * num_eval_steps, '%5d'),
+                ('episode_return', eval_stats['mean_episode_return'], '%2.2f'),
+                ('num_episodes', eval_stats['num_episodes'], '%3d'),
+                ('step_rate', eval_stats['step_rate'], '%4.0f'),
+                ('duration', eval_stats['duration'], '%.2f'),
+            ]
+            log_queue.put(log_output)
+
+        time.sleep(5)
+
+    # Signal actors training session ended.
+    stop_event.set()
+    # Signal logger training session ended, using stop_event seems not working.
+    log_queue.put('PROCESS_DONE')
+
+
+def run_learner_loop(
+    learner: types_lib.Learner,
+    data_queue: multiprocessing.Queue,
+    num_actors: int,
+    learner_trackers: Iterable[Any],
+) -> None:
+    """
+    Run learner loop by constantly pull item off multiprocessing.queue and calls the learner.step() method.
+    """
+
+    num_done_actors = 0
+
+    # Run training steps.
+    while True:
+        # Try to pull one item off multiprocessing.queue.
+        try:
+            item = data_queue.get()
+            if item == 'PROCESS_DONE':  # one actor process is done for current iteration
+                num_done_actors += 1
+            else:
+                learner.received_item_from_queue(item)
+        except queue.Empty:
+            pass
+        except EOFError:
+            pass
+
+        # Only break if all actor processes are done
+        if num_done_actors == num_actors:
+            break
+
+        # The returned stats_sequences could be None when call learner.step(), since it will perform internal checks.
+        stats_sequences = learner.step()
+
+        if stats_sequences is not None:
+            # Some agents may perform multiple network updates in a single call to method step(), like PPO.
+            for stats in stats_sequences:
+                for tracker in learner_trackers:
+                    tracker.step(stats)
+
+
+def run_logger(log_queue: multiprocessing.SimpleQueue, csv_file: str):
+    """Run logger and csv file writer on a separate thread,
+    this is only for training/evaluation statistics."""
+
+    # Create log file writer.
+    writer = CsvWriter(csv_file)
+
+    while True:
+        try:
+            log_output = log_queue.get()
+            if log_output == 'PROCESS_DONE':
+                break
+            log_output_str = ', '.join(('%s: ' + f) % (n, v) for n, v, f in log_output)
+            logging.info(log_output_str)
+            writer.write(collections.OrderedDict((n, v) for n, v, _ in log_output))
+        except queue.Empty:
+            pass
+        except EOFError:
+            pass
+
+
+def run_evaluation_iterations(
+    num_iterations: int,
+    num_eval_steps: int,
+    eval_agent: types_lib.Agent,
+    eval_env: gym.Env,
+    use_tensorboard: bool,
+    recording_video_dir: str = None,
+    root_dir: str = None,
+):
+    """Testing an agent restored from checkpoint.
+
+    Args:
+        num_iterations: number of iterations to run.
+        num_eval_steps: number of evaluation steps, per iteration.
+        eval_agent: evaluation agent, expect the agent has step(), reset(), and agent_name property.
+        eval_env: evaluation environment.
+        use_tensorboard: if True, use tensorboard to log the runs.
+        recording_video_dir: folder to store agent self-play video for one episode.
+    """
+
+    # Tensorboard log dir prefix.
+    # root_dir = "/scratch/nedko_savov/projects/ivg/external/open-genie/agent57"
+    assert root_dir is not None, "root_dir must be provided"
+    test_tb_log_prefix = get_tb_log_prefix(eval_env.spec.id, eval_agent.agent_name, None, 'test') if use_tensorboard else None
+    test_trackers = trackers_lib.make_default_trackers(root_dir, test_tb_log_prefix)
+
+    if num_iterations > 0 and num_eval_steps > 0:
+        for iteration in range(1, num_iterations + 1):
+            logging.info(f'Testing iteration {iteration}')
+
+            # Run some testing steps.
+            eval_stats = run_env_steps(num_eval_steps, eval_agent, eval_env, test_trackers)
+            logging.info(f'Finished run evaluation iteration {iteration}')
+
+            # Logging testing statistics.
+            log_output = [
+                ('iteration', iteration, '%3d'),
+                ('step', iteration * num_eval_steps, '%5d'),
+                ('episode_return', eval_stats['mean_episode_return'], '% 2.2f'),
+                ('num_episodes', eval_stats['num_episodes'], '%3d'),
+                ('step_rate', eval_stats['step_rate'], '%4.0f'),
+                ('duration', eval_stats['duration'], '%.2f'),
+            ]
+
+            log_output_str = ', '.join(('%s: ' + f) % (n, v) for n, v, f in log_output)
+            logging.info(log_output_str)
+            iteration += 1
+
+    if recording_video_dir is not None and recording_video_dir != '':
+        gym_env.play_and_record_video(eval_agent, eval_env, recording_video_dir)
+
+
+def get_tb_log_prefix(env_id: str, agent_name: str, tag: str, suffix: str) -> str:
+    """Returns the composed tensorboard log prefix,
+    which is in the format {env_id}-{agent_name}-{tag}-{suffix}."""
+    tb_log_prefix = f'{env_id}-{agent_name}'
+    if tag is not None and tag != '':
+        tb_log_prefix += f'-{tag}'
+    tb_log_prefix += f'-{suffix}'
+    return tb_log_prefix
+
+
+def init_absl_logging():
+    """Initialize absl.logging when run the process without app.run()"""
+    logging._warn_preinit_stderr = 0  # pylint: disable=protected-access
+    logging.set_verbosity(logging.INFO)
+    logging.use_absl_handler()
+
+
+def handle_exit_signal():
+    """Listen to exit signal like ctrl-c or kill from os and try to exit the process forcefully."""
+
+    def shutdown(signal_code, frame):
+        del frame
+        logging.info(
+            f'Received signal {signal_code}: terminating process...',
+        )
+        sys.exit(128 + signal_code)
+
+    # Listen to signals to exit process.
+    signal.signal(signal.SIGHUP, shutdown)
+    signal.signal(signal.SIGINT, shutdown)
+    signal.signal(signal.SIGTERM, shutdown)
diff --git a/deep_rl_zoo/multi_process_env.py b/deep_rl_zoo/multi_process_env.py
new file mode 100644
index 0000000..edda160
--- /dev/null
+++ b/deep_rl_zoo/multi_process_env.py
@@ -0,0 +1,202 @@
+from dataclasses import astuple, dataclass
+from enum import Enum
+from multiprocessing import Pipe, Process
+from multiprocessing.connection import Connection
+from typing import Any, Callable, Iterator, List, Optional, Tuple
+from lovely_numpy import lo
+import numpy as np
+from einops import rearrange
+import cv2
+import torch
+import torch.nn.functional as F
+# from tools.logger import getLogger
+# log = getLogger(__name__)
+
+from .done_tracker import DoneTrackerEnv
+from .retrowrapper import RetroWrapper, set_retro_make
+import pickle
+
+
+class MessageType(Enum):
+    RESET = 0
+    RESET_RETURN = 1
+    STEP = 2
+    STEP_RETURN = 3
+    CLOSE = 4
+
+
+@dataclass
+class Message:
+    type: MessageType
+    content: Optional[Any] = None
+
+    def __iter__(self) -> Iterator:
+        return iter(astuple(self))
+
+
+def child_env(child_id: int, env_fn: Callable, child_conn: Connection) -> None:
+    
+    np.random.seed(child_id + np.random.randint(0, 2 ** 31 - 1))
+    env = env_fn()
+    while True:
+        message_type, content = child_conn.recv()
+        if message_type == MessageType.RESET:
+            obs = env.reset()
+            child_conn.send(Message(MessageType.RESET_RETURN, obs))
+        elif message_type == MessageType.STEP:
+            obs, rew, terminated, truncated, _ = env.step(content)
+            done = terminated or truncated
+            if terminated or truncated:
+                obs = env.reset()
+            child_conn.send(Message(MessageType.STEP_RETURN, (obs, rew, done, None)))
+        elif message_type == MessageType.CLOSE:
+            child_conn.close()
+            return
+        else:
+            raise NotImplementedError
+
+def process_obs_np(obs, transform):
+    new_obs = []
+    for ob in obs:
+        ob = transform(ob)
+        new_obs.append(ob)
+    return new_obs
+
+class Spec:
+    def __init__(self, id: str) -> None:
+        self.id = id
+
+class MultiProcessEnv(DoneTrackerEnv):
+    def __init__(self, env_fn: Callable, num_envs: int, should_wait_num_envs_ratio: float, transform: int|None =None, id:str="retro") -> None:
+        super().__init__(num_envs)
+        self.transform = transform
+        self.spec = Spec(id)
+        self.env_fn = env_fn
+        # set_retro_make( env_fn )
+        # def env_callback():
+        #     return RetroWrapper(game="Airstriker-Genesis")
+
+        self.grayscale = True
+        self.frame_width = 84
+        self.frame_height = 84
+
+        temp_env = env_fn()
+        self.num_actions = temp_env.action_space.n
+        self.obs_shape = (1, self.frame_width, self.frame_height)#temp_env.observation_space.shape
+        del temp_env
+        # self.num_actions = env_fn().env.action_space.n
+        self.should_wait_num_envs_ratio = should_wait_num_envs_ratio
+        self.processes, self.parent_conns = [], []
+        for child_id in range(num_envs):
+            parent_conn, child_conn = Pipe()
+            self.parent_conns.append(parent_conn)
+            p = Process(target=child_env, args=(child_id, env_fn, child_conn), daemon=True)
+            self.processes.append(p)
+        for p in self.processes:
+            p.start()
+
+    def should_reset(self) -> bool:
+        return (self.num_envs_done / self.num_envs) >= self.should_wait_num_envs_ratio
+
+    def _receive(self, check_type: Optional[MessageType] = None) -> List[Any]:
+        messages = [parent_conn.recv() for parent_conn in self.parent_conns]
+        if check_type is not None:
+            assert all([m.type == check_type for m in messages])
+        return [m.content for m in messages]
+
+    def process_obs(self, obs: Any) -> Any:
+        if self.grayscale:
+            obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
+        obs = cv2.resize(obs, (self.frame_width, self.frame_height), interpolation=cv2.INTER_AREA)
+        # pylint: disable=no-member
+
+        if self.grayscale:
+            obs = np.expand_dims(obs, -1)
+        return obs
+
+    def reset(self) -> np.ndarray:
+        self.reset_done_tracker()
+        for parent_conn in self.parent_conns:
+            parent_conn.send(Message(MessageType.RESET))
+        content = self._receive(check_type=MessageType.RESET_RETURN)
+        content = [c[0] for c in content]
+        if self.transform is not None:
+            for idx in range(len(content)):
+                temp = np.expand_dims(np.array(content[idx]), axis=0)
+                temp = process_obs_np(temp, self.transform)
+                content[idx] = rearrange(temp, 't c h w -> t h w c')[0]
+        else:
+            for idx in range(len(content)):
+                temp = np.array(content[idx])
+                temp = self.process_obs(temp)
+                content[idx] = rearrange(temp, 'h w c -> c h w')
+        return np.stack(content)[0]
+
+    def step(self, actions) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Any]:
+        if type(actions) is not np.ndarray:
+            if type(actions) is list:
+                actions = np.array(actions)
+            else:
+                actions = np.array([actions])
+                
+        actions = F.one_hot(torch.tensor(actions), num_classes=self.num_actions).float().numpy()
+        for parent_conn, action in zip(self.parent_conns, actions):
+            parent_conn.send(Message(MessageType.STEP, action))
+        content = self._receive(check_type=MessageType.STEP_RETURN)
+        obs, rew, done, _ = zip(*content)
+        # done = [t or r for t, r in zip(terminated, truncated)]
+        if isinstance(obs, tuple):
+            obs = list(obs)
+        for idx in range(len(obs)):
+            if isinstance(obs[idx], tuple):
+                obs[idx] = obs[idx][0]
+        done = np.stack(done)
+        self.update_done_tracker(done)
+        
+        new_obs = []
+        if self.transform is not None:
+            for idx in range(len(obs)):
+                try:
+                    temp = np.expand_dims(np.array(obs[idx]), axis=0)
+                    temp = process_obs_np(temp, self.transform)
+                    temp = rearrange(temp, 't c h w -> t h w c')[0]
+                    new_obs.append(temp)
+                except Exception as e:
+                    print("Wut", idx, obs)
+                    #save obs[idx] to file
+                    with open(f'/home/nedko_savov/projects/ivg/external/open-genie/obs_{idx}.pkl', 'wb') as f:
+                        pickle.dump(obs[idx], f)
+                    for o in obs[idx]:
+                        print("W", len(o))
+                        print("H", len(o[0]))
+                        # for o1 in o:
+                        #     log.e(len(o1))
+                    print(f"Error in step: {e}")
+                    raise e
+        else:
+            for idx in range(len(obs)):
+                temp = np.array(obs[idx])
+                temp = self.process_obs(temp)
+                temp = rearrange(temp, 'h w c -> c h w')
+                new_obs.append(temp)
+            # print("No transform")
+            # new_obs = obs
+
+        try:
+            new_obs = np.stack(new_obs)
+        except Exception as e:
+            print("Stack fail", new_obs)
+            # for o in new_obs:
+            #     log.e("W", len(o))
+            #     for o1 in o:
+            #         log.e("H", len(o1))
+            raise Exception(e)
+        return new_obs[0], np.stack(rew)[0], done[0], None
+
+    def close(self) -> None:
+        for parent_conn in self.parent_conns:
+            parent_conn.send(Message(MessageType.CLOSE))
+        for p in self.processes:
+            p.join()
+        for parent_conn in self.parent_conns:
+            parent_conn.close()
diff --git a/deep_rl_zoo/networks/value.py b/deep_rl_zoo/networks/value.py
index 9b48aac..6474d49 100644
--- a/deep_rl_zoo/networks/value.py
+++ b/deep_rl_zoo/networks/value.py
@@ -1186,7 +1186,6 @@ class NguDqnConvNet(nn.Module):
         # last intrinsic reward
         # last extrinsic reward
         core_output_size = self.body.out_features + self.num_policies + self.action_dim + 1 + 1
-
         self.lstm = nn.LSTM(input_size=core_output_size, hidden_size=512, num_layers=1)
 
         self.advantage_head = nn.Sequential(
diff --git a/deep_rl_zoo/retrowrapper.py b/deep_rl_zoo/retrowrapper.py
new file mode 100644
index 0000000..0aad5ca
--- /dev/null
+++ b/deep_rl_zoo/retrowrapper.py
@@ -0,0 +1,151 @@
+"""
+Taken from https://github.com/MaxStrange/retrowrapper/blob/master/retrowrapper.py
+"""
+
+
+"""
+This module exposes the RetroWrapper class.
+"""
+import multiprocessing
+import retro
+import gc
+
+MAKE_RETRIES = 5
+
+def set_retro_make( new_retro_make_func ):
+    RetroWrapper.retro_make_func = new_retro_make_func
+
+def _retrocom(rx, tx, game, kwargs):
+    """
+    This function is the target for RetroWrapper's internal
+    process and does all the work of communicating with the
+    environment.
+    """
+    env = RetroWrapper.retro_make_func(game, **kwargs)
+
+    # Sit around on the queue, waiting for calls from RetroWrapper
+    while True:
+        attr, args, kwargs = rx.get()
+
+        # First, handle special case where the wrapper is asking if attr is callable.
+        # In this case, we actually have RetroWrapper.symbol, attr, and {}.
+        if attr == RetroWrapper.symbol:
+            result = env.__getattribute__(args)
+            tx.put(callable(result))
+        elif attr == "close":
+            env.close()
+            break
+        else:
+            # Otherwise, handle the request
+            result = getattr(env, attr)
+            if callable(result):
+                result = result(*args, **kwargs)
+            tx.put(result)
+
+
+class RetroWrapper():
+    """
+    This class is a thin wrapper around a retro environment.
+
+    The purpose of this class is to protect us from the fact
+    that each Python process can only have a single retro
+    environment at a time, and we would like potentially
+    several.
+
+    This class gets around this limitation by spawning a process
+    internally that sits around waiting for retro environment
+    API calls, asking its own local copy of the environment, and
+    then returning the answer.
+
+    Call functions on this object exactly as if it were a retro env.
+    """
+    symbol = "THIS IS A SPECIAL MESSAGE FOR YOU"
+    retro_make_func = retro.make
+
+    def __init__(self, game, **kwargs):
+        tempenv = None
+        retry_counter = MAKE_RETRIES
+        while True:
+            try:
+                tempenv = RetroWrapper.retro_make_func(game, **kwargs)
+            except RuntimeError: # Sometimes we need to gc.collect because previous tempenvs haven't been cleaned up.
+                gc.collect()
+                retry_counter -= 1
+                if retry_counter > 0:
+                    continue
+            break
+
+        if tempenv == None:
+            raise RuntimeError( 'Unable to create tempenv' )
+
+        tempenv.reset()
+
+        if hasattr( tempenv, 'unwrapped' ): # Wrappers don't have gamename or initial_state
+            tempenv_unwrapped = tempenv.unwrapped
+            self.gamename = tempenv_unwrapped.gamename
+            self.initial_state = tempenv_unwrapped.initial_state
+
+        self.action_space = tempenv.action_space
+        self.metadata = tempenv.metadata
+        self.observation_space = tempenv.observation_space
+        self.reward_range = tempenv.reward_range
+        tempenv.close()
+
+        self._rx = multiprocessing.Queue()
+        self._tx = multiprocessing.Queue()
+        self._proc = multiprocessing.Process(target=_retrocom, args=(self._tx, self._rx, game, kwargs), daemon=True)
+        self._proc.start()
+
+    def __del__(self):
+        """
+        Make sure to clean up.
+        """
+        self.close()
+
+    def __getattr__(self, attr):
+        """
+        Any time a client calls anything on our object, we want to check to
+        see if we can answer without having to ask the retro process. Usually,
+        we will have to ask it. If we do, we put a request into the queue for the
+        result of whatever the client requested and block until it comes back.
+
+        Otherwise we simply give the client whatever we have that they want.
+
+        BTW: This doesn't work for magic methods. To get those working is a little more involved. TODO
+        """
+        # E.g.: Client calls env.step(action)
+        ignore_list = ['class', 'mro', 'new', 'init', 'setattr', 'getattr', 'getattribute']
+        if attr in self.__dict__ and attr not in ignore_list:
+            # 1. Check if we have a step function. If so, return it.
+            return attr
+        else:
+            # 2. If we don't, return a function that calls step with whatever args are passed in to it.
+            is_callable = self._ask_if_attr_is_callable(attr)
+
+            if is_callable:
+                # The result of getattr(attr) is a callable, so return a wrapper
+                # that pretends to be the function the user was trying to call
+                def wrapper(*args, **kwargs):
+                    self._tx.put((attr, args, kwargs))
+                    return self._rx.get()
+                return wrapper
+            else:
+                # The result of getattr(attr) is not a callable, so we should just
+                # execute the request for the user and return the result
+                self._tx.put((attr, [], {}))
+                return self._tx.get()
+
+    def _ask_if_attr_is_callable(self, attr):
+        """
+        Returns whether or not the attribute is a callable.
+        """
+        self._tx.put((RetroWrapper.symbol, attr, {}))
+        return self._rx.get()
+
+    def close(self):
+        """
+        Shutdown the environment.
+        """
+        if "_tx" in self.__dict__ and "_proc" in self.__dict__:
+            self._tx.put(("close", (), {}))
+            self._proc.join()
\ No newline at end of file
diff --git a/deep_rl_zoo/trackers.py b/deep_rl_zoo/trackers.py
index 4bdb5cb..8e64b30 100644
--- a/deep_rl_zoo/trackers.py
+++ b/deep_rl_zoo/trackers.py
@@ -323,7 +323,7 @@ class TensorboardLearnerStatisticsTracker:
         }
 
 
-def make_default_trackers(log_dir=None, debug_screenshots_interval=0):
+def make_default_trackers(root_dir,log_dir=None, debug_screenshots_interval=0):
     """
     Create trackers for the training/evaluation run.
 
@@ -333,7 +333,7 @@ def make_default_trackers(log_dir=None, debug_screenshots_interval=0):
     """
 
     if log_dir:
-        log_dir = Path(f'runs/{log_dir}')
+        log_dir = Path(f'{root_dir}/runs/{log_dir}')
 
         # Remove existing log directory
         if log_dir.exists() and log_dir.is_dir():
@@ -356,7 +356,7 @@ def make_default_trackers(log_dir=None, debug_screenshots_interval=0):
         return [EpisodeTracker(), StepRateTracker()]
 
 
-def make_learner_trackers(run_log_dir=None):
+def make_learner_trackers(root_dir, run_log_dir=None):
     """
     Create trackers for learner for parallel training (actor-learner) run.
 
@@ -365,7 +365,7 @@ def make_learner_trackers(run_log_dir=None):
     """
 
     if run_log_dir:
-        tb_log_dir = Path(f'runs/{run_log_dir}')
+        tb_log_dir = Path(f'{root_dir}/runs/{run_log_dir}')
 
         # Remove existing log directory
         if tb_log_dir.exists() and tb_log_dir.is_dir():
